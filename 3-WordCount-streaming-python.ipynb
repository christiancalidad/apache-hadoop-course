{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop/MapReduce -- WordCount en Python (Hadoop Streaming)\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/big-data-analytics/tree/master/) para acceder al repositorio online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/big-data-analytics/tree/master/) para explorar el repositorio usando `nbviewer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del problema\n",
    "\n",
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos. Debido a los requerimientos de diseño (gran volúmen de datos y tiempos rápidos de respuesta) se desea implementar una arquitectura Big Data. Se desea implementar la solución en **Python** y ejecutar Hadoop en **modo pseudo-distribuido**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se crea el directorio de entrada\n",
    "!rm -rf input\n",
    "!rm -rf output\n",
    "!mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de la implementación\n",
    "\n",
    "Para realizar la implementación solicitada se usará Hadoop Streaming, el cual permite la implementación de aplicaciones MapReduce en cualquier lenguaje que soporte la creación de scripts. Para este caso se ejemplificará la implementación en Python. Hadoop streaming debe su nombre a que usa los streamings de Unix para la entrada y salida en texto (stdin, stdout y stderr respectivamente). El sistema lee y escribe líneas de texto una a una a los respectivos flujos de entrada y salida, tal como ocurre de forma estándar en las herramientas de la línea de comandos de Linux. \n",
    "\n",
    "La implementación requiere un programa para realizar el mapeo y otro para la reducción que se ejecutan independientemente; Hadoop se encarga de la coordinación entre ellos. El intercambio de información se hace a traves de texto, que es el lenguaje universal para intercambio de información. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1\n",
    "\n",
    "Se implementa la función map en Python y se guarda en el archivo `mapper.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#! /usr/bin/env python\n",
    "\n",
    "##\n",
    "## Esta es la función que mapea la entrada a parejas (clave, valor)\n",
    "##\n",
    "import sys\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    ##\n",
    "    ## itera sobre cada linea de código recibida\n",
    "    ## a través del flujo de entrada\n",
    "    ##\n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        ##\n",
    "        ## genera las tuplas palabra \\tabulador 1\n",
    "        ## ya que es un conteo de palabras\n",
    "        ##\n",
    "        for word in line.split(): \n",
    "                   \n",
    "            ##\n",
    "            ## escribe al flujo estándar de salida\n",
    "            ##\n",
    "            sys.stdout.write(\"{}\\t1\\n\".format(word))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## El programa anterior se hace ejecutable\n",
    "!chmod +x mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics\t1\n",
      "is\t1\n",
      "the\t1\n",
      "discovery,\t1\n",
      "interpretation,\t1\n",
      "and\t1\n",
      "communication\t1\n",
      "of\t1\n",
      "meaningful\t1\n",
      "patterns\t1\n"
     ]
    }
   ],
   "source": [
    "## la salida de la función anterior es:\n",
    "!cat ./input/text*.txt | ./mapper.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3\n",
    "\n",
    "Se implementa la función `reduce` y se salva en el archivo `reducer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "##\n",
    "## Esta función reduce los elementos que \n",
    "## tienen la misma clave\n",
    "##\n",
    "\n",
    "if __name__ == '__main__': \n",
    "  \n",
    "    curkey = None\n",
    "    total = 0\n",
    "    \n",
    "    ##\n",
    "    ## cada linea de texto recibida es una \n",
    "    ## entrada clave \\tabulador valor\n",
    "    ##\n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        key, val = line.split(\"\\t\") \n",
    "        val = int(val)\n",
    "        \n",
    "        if key == curkey: \n",
    "            ##\n",
    "            ## No se ha cambiado de clave. Aca se \n",
    "            ## acumulan los valores para la misma clave.\n",
    "            ##\n",
    "            total += val  \n",
    "        else:\n",
    "            ##\n",
    "            ## Se cambio de clave. Se reinicia el\n",
    "            ## acumulador.\n",
    "            ##\n",
    "            if curkey is not None:\n",
    "                ##\n",
    "                ## una vez se han reducido todos los elementos\n",
    "                ## con la misma clave se imprime el resultado en\n",
    "                ## el flujo de salida\n",
    "                ##\n",
    "                sys.stdout.write(\"{}\\t{}\\n\".format(curkey, total)) \n",
    "            \n",
    "            curkey = key\n",
    "            total = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## El archivo se hace ejecutable\n",
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5\n",
    "\n",
    "Se realiza la prueba de la implementación en el directorio actual antes de realizar la ejecución en el modo pseudo-distribuido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t2\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Specifically,\t1\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## La función sort hace que todos los elementos con \n",
    "## la misma clave queden en lineas consecutivas.\n",
    "## Hace el papel del módulo Shuffle & Sort\n",
    "##\n",
    "!cat ./input/text*.txt | ./mapper.py | sort | ./reducer.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecución en Hadoop\n",
    "\n",
    "Una vez se tienen las versiones anteriores funcionando, se puede proceder a ejecutar la tarea directamente en Hadoop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se elimina el directorio de salida\n",
    "!rm -rf output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5\n",
    "\n",
    "Se copian los archivos de entrada al HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:48:04,962 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## crea el directorio para los archivos de entrada\n",
    "!$HADOOP_HOME/bin/hadoop fs -mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:48:08,175 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## Copia el contenido de la carpeta input a la carpeta input en el HDFS\n",
    "!$HADOOP_HOME/bin/hadoop fs -put  input/* input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:48:12,360 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jdvelasq supergroup       1092 2018-09-02 20:48 input/text0.txt\n",
      "-rw-r--r--   1 jdvelasq supergroup        351 2018-09-02 20:48 input/text1.txt\n",
      "-rw-r--r--   1 jdvelasq supergroup        439 2018-09-02 20:48 input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "## verifica el contenido del directorio\n",
    "!$HADOOP_HOME/bin/hadoop fs -ls input/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 6\n",
    "\n",
    "Se ejecuta Hadoop Streaming en modo pseudo-distribuido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:50:50,710 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-09-02 20:50:51,634 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "2018-09-02 20:50:51,689 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2018-09-02 20:50:51,689 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2018-09-02 20:50:51,701 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-09-02 20:50:52,075 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2018-09-02 20:50:52,126 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2018-09-02 20:50:52,248 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2123453368_0001\n",
      "2018-09-02 20:50:52,249 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2018-09-02 20:50:52,348 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2018-09-02 20:50:52,349 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2018-09-02 20:50:52,350 INFO mapreduce.Job: Running job: job_local2123453368_0001\n",
      "2018-09-02 20:50:52,350 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2018-09-02 20:50:52,354 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 20:50:52,354 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 20:50:52,380 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2018-09-02 20:50:52,382 INFO mapred.LocalJobRunner: Starting task: attempt_local2123453368_0001_m_000000_0\n",
      "2018-09-02 20:50:52,400 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 20:50:52,400 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 20:50:52,407 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 20:50:52,407 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 20:50:52,412 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jdvelasq/input/text0.txt:0+1092\n",
      "2018-09-02 20:50:52,426 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-09-02 20:50:52,489 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-09-02 20:50:52,489 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-09-02 20:50:52,489 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-09-02 20:50:52,489 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-09-02 20:50:52,489 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-09-02 20:50:52,491 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-09-02 20:50:52,499 INFO streaming.PipeMapRed: PipeMapRed exec [/Volumes/JetDrive/GitHub/apache-hadoop-course/./mapper.py]\n",
      "2018-09-02 20:50:52,502 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2018-09-02 20:50:52,502 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2018-09-02 20:50:52,503 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2018-09-02 20:50:52,503 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2018-09-02 20:50:52,503 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2018-09-02 20:50:52,503 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2018-09-02 20:50:52,503 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2018-09-02 20:50:52,504 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-09-02 20:50:52,504 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2018-09-02 20:50:52,504 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2018-09-02 20:50:52,504 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2018-09-02 20:50:52,504 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2018-09-02 20:50:52,577 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-09-02 20:50:52,577 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-09-02 20:50:52,579 INFO streaming.PipeMapRed: Records R/W=14/1\n",
      "2018-09-02 20:50:52,582 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-09-02 20:50:52,582 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-09-02 20:50:52,584 INFO mapred.LocalJobRunner: \n",
      "2018-09-02 20:50:52,584 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-09-02 20:50:52,584 INFO mapred.MapTask: Spilling map output\n",
      "2018-09-02 20:50:52,584 INFO mapred.MapTask: bufstart = 0; bufend = 1347; bufvoid = 104857600\n",
      "2018-09-02 20:50:52,584 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213868(104855472); length = 529/6553600\n",
      "2018-09-02 20:50:52,606 INFO mapred.MapTask: Finished spill 0\n",
      "2018-09-02 20:50:52,620 INFO mapred.Task: Task:attempt_local2123453368_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-09-02 20:50:52,623 INFO mapred.LocalJobRunner: Records R/W=14/1\n",
      "2018-09-02 20:50:52,623 INFO mapred.Task: Task 'attempt_local2123453368_0001_m_000000_0' done.\n",
      "2018-09-02 20:50:52,630 INFO mapred.Task: Final Counters for attempt_local2123453368_0001_m_000000_0: Counters: 22\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=176716\n",
      "\t\tFILE: Number of bytes written=683118\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1092\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=5\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=14\n",
      "\t\tMap output records=133\n",
      "\t\tMap output bytes=1347\n",
      "\t\tMap output materialized bytes=1619\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=133\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=321388544\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1092\n",
      "2018-09-02 20:50:52,630 INFO mapred.LocalJobRunner: Finishing task: attempt_local2123453368_0001_m_000000_0\n",
      "2018-09-02 20:50:52,631 INFO mapred.LocalJobRunner: Starting task: attempt_local2123453368_0001_m_000001_0\n",
      "2018-09-02 20:50:52,632 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 20:50:52,632 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 20:50:52,633 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 20:50:52,633 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 20:50:52,634 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jdvelasq/input/text2.txt:0+439\n",
      "2018-09-02 20:50:52,637 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-09-02 20:50:52,696 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-09-02 20:50:52,696 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-09-02 20:50:52,696 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-09-02 20:50:52,696 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-09-02 20:50:52,696 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-09-02 20:50:52,696 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-09-02 20:50:52,702 INFO streaming.PipeMapRed: PipeMapRed exec [/Volumes/JetDrive/GitHub/apache-hadoop-course/./mapper.py]\n",
      "2018-09-02 20:50:52,714 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-09-02 20:50:52,753 INFO streaming.PipeMapRed: Records R/W=6/1\n",
      "2018-09-02 20:50:52,756 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-09-02 20:50:52,756 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-09-02 20:50:52,757 INFO mapred.LocalJobRunner: \n",
      "2018-09-02 20:50:52,757 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-09-02 20:50:52,757 INFO mapred.MapTask: Spilling map output\n",
      "2018-09-02 20:50:52,757 INFO mapred.MapTask: bufstart = 0; bufend = 559; bufvoid = 104857600\n",
      "2018-09-02 20:50:52,757 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600\n",
      "2018-09-02 20:50:52,768 INFO mapred.MapTask: Finished spill 0\n",
      "2018-09-02 20:50:52,776 INFO mapred.Task: Task:attempt_local2123453368_0001_m_000001_0 is done. And is in the process of committing\n",
      "2018-09-02 20:50:52,778 INFO mapred.LocalJobRunner: Records R/W=6/1\n",
      "2018-09-02 20:50:52,778 INFO mapred.Task: Task 'attempt_local2123453368_0001_m_000001_0' done.\n",
      "2018-09-02 20:50:52,779 INFO mapred.Task: Final Counters for attempt_local2123453368_0001_m_000001_0: Counters: 22\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=177044\n",
      "\t\tFILE: Number of bytes written=683839\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1531\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=7\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=62\n",
      "\t\tMap output bytes=559\n",
      "\t\tMap output materialized bytes=689\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=62\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=426770432\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=439\n",
      "2018-09-02 20:50:52,779 INFO mapred.LocalJobRunner: Finishing task: attempt_local2123453368_0001_m_000001_0\n",
      "2018-09-02 20:50:52,779 INFO mapred.LocalJobRunner: Starting task: attempt_local2123453368_0001_m_000002_0\n",
      "2018-09-02 20:50:52,780 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 20:50:52,780 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 20:50:52,780 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 20:50:52,780 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 20:50:52,781 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jdvelasq/input/text1.txt:0+351\n",
      "2018-09-02 20:50:52,783 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-09-02 20:50:52,845 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-09-02 20:50:52,845 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-09-02 20:50:52,845 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-09-02 20:50:52,845 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-09-02 20:50:52,845 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-09-02 20:50:52,846 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-09-02 20:50:52,850 INFO streaming.PipeMapRed: PipeMapRed exec [/Volumes/JetDrive/GitHub/apache-hadoop-course/./mapper.py]\n",
      "2018-09-02 20:50:52,860 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-09-02 20:50:52,897 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "2018-09-02 20:50:52,900 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-09-02 20:50:52,900 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-09-02 20:50:52,901 INFO mapred.LocalJobRunner: \n",
      "2018-09-02 20:50:52,901 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-09-02 20:50:52,901 INFO mapred.MapTask: Spilling map output\n",
      "2018-09-02 20:50:52,901 INFO mapred.MapTask: bufstart = 0; bufend = 463; bufvoid = 104857600\n",
      "2018-09-02 20:50:52,901 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214172(104856688); length = 225/6553600\n",
      "2018-09-02 20:50:52,911 INFO mapred.MapTask: Finished spill 0\n",
      "2018-09-02 20:50:52,918 INFO mapred.Task: Task:attempt_local2123453368_0001_m_000002_0 is done. And is in the process of committing\n",
      "2018-09-02 20:50:52,921 INFO mapred.LocalJobRunner: Records R/W=4/1\n",
      "2018-09-02 20:50:52,921 INFO mapred.Task: Task 'attempt_local2123453368_0001_m_000002_0' done.\n",
      "2018-09-02 20:50:52,921 INFO mapred.Task: Final Counters for attempt_local2123453368_0001_m_000002_0: Counters: 22\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=177372\n",
      "\t\tFILE: Number of bytes written=684454\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1882\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=57\n",
      "\t\tMap output bytes=463\n",
      "\t\tMap output materialized bytes=583\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=57\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=532152320\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=351\n",
      "2018-09-02 20:50:52,921 INFO mapred.LocalJobRunner: Finishing task: attempt_local2123453368_0001_m_000002_0\n",
      "2018-09-02 20:50:52,921 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2018-09-02 20:50:52,924 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2018-09-02 20:50:52,924 INFO mapred.LocalJobRunner: Starting task: attempt_local2123453368_0001_r_000000_0\n",
      "2018-09-02 20:50:52,930 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 20:50:52,930 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 20:50:52,931 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 20:50:52,931 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 20:50:52,933 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@f38fa6b\n",
      "2018-09-02 20:50:52,934 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-09-02 20:50:52,950 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-09-02 20:50:52,952 INFO reduce.EventFetcher: attempt_local2123453368_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-09-02 20:50:52,972 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2123453368_0001_m_000002_0 decomp: 579 len: 583 to MEMORY\n",
      "2018-09-02 20:50:52,975 INFO reduce.InMemoryMapOutput: Read 579 bytes from map-output for attempt_local2123453368_0001_m_000002_0\n",
      "2018-09-02 20:50:52,976 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 579, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->579\n",
      "2018-09-02 20:50:52,978 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2123453368_0001_m_000000_0 decomp: 1615 len: 1619 to MEMORY\n",
      "2018-09-02 20:50:52,978 INFO reduce.InMemoryMapOutput: Read 1615 bytes from map-output for attempt_local2123453368_0001_m_000000_0\n",
      "2018-09-02 20:50:52,978 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1615, inMemoryMapOutputs.size() -> 2, commitMemory -> 579, usedMemory ->2194\n",
      "2018-09-02 20:50:52,979 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2123453368_0001_m_000001_0 decomp: 685 len: 689 to MEMORY\n",
      "2018-09-02 20:50:52,980 INFO reduce.InMemoryMapOutput: Read 685 bytes from map-output for attempt_local2123453368_0001_m_000001_0\n",
      "2018-09-02 20:50:52,980 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 685, inMemoryMapOutputs.size() -> 3, commitMemory -> 2194, usedMemory ->2879\n",
      "2018-09-02 20:50:52,980 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2018-09-02 20:50:52,981 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-09-02 20:50:52,981 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-09-02 20:50:53,002 INFO mapred.Merger: Merging 3 sorted segments\n",
      "2018-09-02 20:50:53,003 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 2853 bytes\n",
      "2018-09-02 20:50:53,012 INFO reduce.MergeManagerImpl: Merged 3 segments, 2879 bytes to disk to satisfy reduce memory limit\n",
      "2018-09-02 20:50:53,013 INFO reduce.MergeManagerImpl: Merging 1 files, 2879 bytes from disk\n",
      "2018-09-02 20:50:53,013 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-09-02 20:50:53,013 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-09-02 20:50:53,014 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2868 bytes\n",
      "2018-09-02 20:50:53,014 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-09-02 20:50:53,021 INFO streaming.PipeMapRed: PipeMapRed exec [/Volumes/JetDrive/GitHub/apache-hadoop-course/./reducer.py]\n",
      "2018-09-02 20:50:53,023 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-09-02 20:50:53,023 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2018-09-02 20:50:53,060 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-09-02 20:50:53,061 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-09-02 20:50:53,062 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-09-02 20:50:53,078 INFO streaming.PipeMapRed: Records R/W=252/1\n",
      "2018-09-02 20:50:53,081 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-09-02 20:50:53,081 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-09-02 20:50:53,116 INFO mapred.Task: Task:attempt_local2123453368_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-09-02 20:50:53,117 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-09-02 20:50:53,117 INFO mapred.Task: Task attempt_local2123453368_0001_r_000000_0 is allowed to commit now\n",
      "2018-09-02 20:50:53,126 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2123453368_0001_r_000000_0' to hdfs://localhost:9000/user/jdvelasq/output\n",
      "2018-09-02 20:50:53,127 INFO mapred.LocalJobRunner: Records R/W=252/1 > reduce\n",
      "2018-09-02 20:50:53,127 INFO mapred.Task: Task 'attempt_local2123453368_0001_r_000000_0' done.\n",
      "2018-09-02 20:50:53,127 INFO mapred.Task: Final Counters for attempt_local2123453368_0001_r_000000_0: Counters: 29\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=183238\n",
      "\t\tFILE: Number of bytes written=687333\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1882\n",
      "\t\tHDFS: Number of bytes written=1640\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=2891\n",
      "\t\tReduce input records=252\n",
      "\t\tReduce output records=158\n",
      "\t\tSpilled Records=252\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=563085312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1640\n",
      "2018-09-02 20:50:53,128 INFO mapred.LocalJobRunner: Finishing task: attempt_local2123453368_0001_r_000000_0\n",
      "2018-09-02 20:50:53,128 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2018-09-02 20:50:53,353 INFO mapreduce.Job: Job job_local2123453368_0001 running in uber mode : false\n",
      "2018-09-02 20:50:53,354 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2018-09-02 20:50:53,356 INFO mapreduce.Job: Job job_local2123453368_0001 completed successfully\n",
      "2018-09-02 20:50:53,364 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=714370\n",
      "\t\tFILE: Number of bytes written=2738744\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6387\n",
      "\t\tHDFS: Number of bytes written=1640\n",
      "\t\tHDFS: Number of read operations=35\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24\n",
      "\t\tMap output records=252\n",
      "\t\tMap output bytes=2369\n",
      "\t\tMap output materialized bytes=2891\n",
      "\t\tInput split bytes=309\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=2891\n",
      "\t\tReduce input records=252\n",
      "\t\tReduce output records=158\n",
      "\t\tSpilled Records=504\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=1843396608\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1882\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1640\n",
      "2018-09-02 20:50:53,364 INFO streaming.StreamJob: Output directory: output\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se ejecuta en Hadoop.\n",
    "##   -input: archivo de entrada\n",
    "##   -output: directorio de salida\n",
    "##   -maper: programa que ejecuta el map\n",
    "##   -reducer: programa que ejecuta la reducción\n",
    "##\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar -input input -output output  -mapper mapper.py -reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:52:00,571 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 jdvelasq supergroup          0 2018-09-02 20:50 output/_SUCCESS\n",
      "-rw-r--r--   1 jdvelasq supergroup       1640 2018-09-02 20:50 output/part-00000\n"
     ]
    }
   ],
   "source": [
    "## contenido del directorio con los \n",
    "## resultados de la corrida\n",
    "!$HADOOP_HOME/bin/hadoop fs -ls output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:53:25,411 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Specifically,\t1\n"
     ]
    }
   ],
   "source": [
    "## se visualiza el archivo con los\n",
    "## resultados de la corrida\n",
    "!$HADOOP_HOME/bin/hadoop fs -cat output/part-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:54:29,159 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted input/text0.txt\n",
      "Deleted input/text1.txt\n",
      "Deleted input/text2.txt\n",
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-00000\n",
      "2018-09-02 20:54:30,890 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!rm reducer.py mapper.py\n",
    "!rm -rf input\n",
    "!$HADOOP_HOME/bin/hadoop fs -rm input/* output/*\n",
    "!$HADOOP_HOME/bin/hadoop fs -rmdir input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas.\n",
    "\n",
    "**Combiners.--** Los combiners son *reducers* que se ejecutan sobre los resultdos que produce cada mapper antes de pasar al modulo de suffle-&-sort, con el fin de reducir la carga computacional. Suelen ser identicos a los *reducers*.\n",
    "\n",
    "**Partitioners.--** Son rutinas que controlan como se enviar las parejas (clave, valor) a cada reducers, tal que elementos con la misma clave son enviados al mismo reducer. \n",
    "\n",
    "**Job Chain.--** Se refiere al encadenamiento de varias tareas cuando el cómputo que se desea realizar es muy complejo para que pueda realizarse en un MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop/MapReduce -- WordCount en Python (Hadoop Streaming)\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/big-data-analytics/tree/master/) para acceder al repositorio online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/big-data-analytics/tree/master/) para explorar el repositorio usando `nbviewer`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
