{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop/MapReduce -- WordCount en Java\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/big-data-analytics/tree/master/) para acceder al repositorio online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/big-data-analytics/tree/master/) para explorar el repositorio usando `nbviewer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del problema\n",
    "\n",
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos. Debido a los requerimientos de diseño (gran volúmen de datos y tiempos rápidos de respuesta) se desea implementar una arquitectura Big Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se crea el directorio de entrada\n",
    "!rm -rf input\n",
    "!rm -rf output\n",
    "!mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tutorial detallado se encuentra en http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se planteará la solución en Java. Para construir una aplicación que usa MapReduce, el usuario debe implementar la función map y la función reduce; el sistema se encarga por si solo de ejecutarlas en el cluster. La implementación es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount.java\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.java\n",
    "\n",
    "import java.io.IOException;\n",
    "\n",
    "/*\n",
    " * Esta clase permite separar una frase (texto)\n",
    " * en las palabras que lo conforman. La lista\n",
    " * resultante puede ser iterada en un ciclo for\n",
    " */\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "/*\n",
    " *\n",
    " * Librerias requeridas para ejecutar Hadoop\n",
    " *\n",
    " */\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "/*\n",
    " * Esta clase implementa el mapper y el reducer\n",
    " */\n",
    "public class WordCount {\n",
    "\n",
    "  public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "       \n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "\n",
    "    /* \n",
    "     * en esta variable se guarda cada palabra leida        \n",
    "     * del flujo de entrada\n",
    "     */     \n",
    "    private Text word = new Text();\n",
    "\n",
    "    /* \n",
    "     * Este es el mapper. Para cada palabra \n",
    "     * leída, emite el par <word, 1>\n",
    "     */\n",
    "    public void map(Object key,       // Clave\n",
    "                    Text value,       // La linea de texto\n",
    "                    Context context   // Aplicación que se esta ejecutando\n",
    "                    ) throws IOException, InterruptedException {\n",
    "                              \n",
    "      // Convierte la línea de texto en una lista de strings\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "                              \n",
    "      // Ejecuta el ciclo para cada palabra \n",
    "      // de la lista de strings\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        // obtiene la palabra\n",
    "        word.set(itr.nextToken());\n",
    "\n",
    "        // escribe la pareja <word, 1> \n",
    "        // al flujo de salida\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "           \n",
    "    // Clase para imprimir un entero al flujo de salida       \n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    // Esta función es llamada para reducir \n",
    "    // una lista de valores que tienen la misma clave\n",
    "    public void reduce(Text key,                      // clave\n",
    "                       Iterable<IntWritable> values,  // lista de valores\n",
    "                       Context context                // Aplicación que se esta ejecutando\n",
    "                       ) throws IOException, InterruptedException {\n",
    "        \n",
    "      // itera sobre la lista de valores, sumandolos\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "        \n",
    "      // escribe la pareja <word, valor> al flujo\n",
    "      // de salida\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "    \n",
    "  /*\n",
    "   * Se crea la aplicación en Hadoop y se ejecuta\n",
    "   */\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    \n",
    "    /*\n",
    "     * El job corresponde a la aplicacion\n",
    "     */\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "      \n",
    "    /*\n",
    "     * La clase que contiene el mapper y el reducer\n",
    "     */\n",
    "    job.setJarByClass(WordCount.class);\n",
    "      \n",
    "    /* \n",
    "     * Clase que implementa el mapper  \n",
    "     */ \n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "      \n",
    "    /*\n",
    "     * El combiner es un reducer que se coloca a la salida\n",
    "     * del mapper para agilizar el computo\n",
    "     */\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    \n",
    "    /*\n",
    "     * Clase que implementa el reducer\n",
    "     */\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "      \n",
    "    /*\n",
    "     * Salida\n",
    "     */\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    \n",
    "    /*\n",
    "     * Formatos de entrada y salida\n",
    "     */\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "   \n",
    "    // resultado de la ejecución.\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount.java\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.java\n",
    "\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "  public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(Object key, Text value, Context context\n",
    "                    ) throws IOException, InterruptedException {\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        word.set(itr.nextToken());\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    public void reduce(Text key, Iterable<IntWritable> values,\n",
    "                       Context context\n",
    "                       ) throws IOException, InterruptedException {\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "    job.setJarByClass(WordCount.class);\n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-27 16:25:12,169 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-08-27 16:25:12,567 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "2018-08-27 16:25:12,776 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2018-08-27 16:25:12,776 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2018-08-27 16:25:12,965 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "2018-08-27 16:25:13,062 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2018-08-27 16:25:13,116 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2018-08-27 16:25:13,264 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local560905394_0001\n",
      "2018-08-27 16:25:13,266 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2018-08-27 16:25:13,376 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2018-08-27 16:25:13,377 INFO mapreduce.Job: Running job: job_local560905394_0001\n",
      "2018-08-27 16:25:13,378 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2018-08-27 16:25:13,383 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-08-27 16:25:13,383 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-08-27 16:25:13,383 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2018-08-27 16:25:13,419 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2018-08-27 16:25:13,420 INFO mapred.LocalJobRunner: Starting task: attempt_local560905394_0001_m_000000_0\n",
      "2018-08-27 16:25:13,437 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-08-27 16:25:13,437 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-08-27 16:25:13,445 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-08-27 16:25:13,445 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-08-27 16:25:13,448 INFO mapred.MapTask: Processing split: file:/Volumes/JetDrive/GitHub/big-data-analytics/input/text0.txt:0+1092\n",
      "2018-08-27 16:25:13,513 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-08-27 16:25:13,513 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-08-27 16:25:13,513 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-08-27 16:25:13,513 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-08-27 16:25:13,513 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-08-27 16:25:13,517 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-08-27 16:25:13,524 INFO mapred.LocalJobRunner: \n",
      "2018-08-27 16:25:13,524 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-08-27 16:25:13,524 INFO mapred.MapTask: Spilling map output\n",
      "2018-08-27 16:25:13,524 INFO mapred.MapTask: bufstart = 0; bufend = 1613; bufvoid = 104857600\n",
      "2018-08-27 16:25:13,524 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213868(104855472); length = 529/6553600\n",
      "2018-08-27 16:25:13,552 INFO mapred.MapTask: Finished spill 0\n",
      "2018-08-27 16:25:13,564 INFO mapred.Task: Task:attempt_local560905394_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-08-27 16:25:13,566 INFO mapred.LocalJobRunner: map\n",
      "2018-08-27 16:25:13,566 INFO mapred.Task: Task 'attempt_local560905394_0001_m_000000_0' done.\n",
      "2018-08-27 16:25:13,571 INFO mapred.Task: Final Counters for attempt_local560905394_0001_m_000000_0: Counters: 18\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4644\n",
      "\t\tFILE: Number of bytes written=500152\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=14\n",
      "\t\tMap output records=133\n",
      "\t\tMap output bytes=1613\n",
      "\t\tMap output materialized bytes=1374\n",
      "\t\tInput split bytes=129\n",
      "\t\tCombine input records=133\n",
      "\t\tCombine output records=95\n",
      "\t\tSpilled Records=95\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=197132288\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1092\n",
      "2018-08-27 16:25:13,571 INFO mapred.LocalJobRunner: Finishing task: attempt_local560905394_0001_m_000000_0\n",
      "2018-08-27 16:25:13,572 INFO mapred.LocalJobRunner: Starting task: attempt_local560905394_0001_m_000001_0\n",
      "2018-08-27 16:25:13,573 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-08-27 16:25:13,573 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-08-27 16:25:13,573 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-08-27 16:25:13,573 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-08-27 16:25:13,574 INFO mapred.MapTask: Processing split: file:/Volumes/JetDrive/GitHub/big-data-analytics/input/text2.txt:0+439\n",
      "2018-08-27 16:25:13,628 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-08-27 16:25:13,628 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-08-27 16:25:13,628 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-08-27 16:25:13,628 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-08-27 16:25:13,628 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-08-27 16:25:13,628 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-08-27 16:25:13,630 INFO mapred.LocalJobRunner: \n",
      "2018-08-27 16:25:13,630 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-08-27 16:25:13,630 INFO mapred.MapTask: Spilling map output\n",
      "2018-08-27 16:25:13,630 INFO mapred.MapTask: bufstart = 0; bufend = 683; bufvoid = 104857600\n",
      "2018-08-27 16:25:13,630 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600\n",
      "2018-08-27 16:25:13,643 INFO mapred.MapTask: Finished spill 0\n",
      "2018-08-27 16:25:13,649 INFO mapred.Task: Task:attempt_local560905394_0001_m_000001_0 is done. And is in the process of committing\n",
      "2018-08-27 16:25:13,650 INFO mapred.LocalJobRunner: map\n",
      "2018-08-27 16:25:13,650 INFO mapred.Task: Task 'attempt_local560905394_0001_m_000001_0' done.\n",
      "2018-08-27 16:25:13,651 INFO mapred.Task: Final Counters for attempt_local560905394_0001_m_000001_0: Counters: 18\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5489\n",
      "\t\tFILE: Number of bytes written=500865\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=62\n",
      "\t\tMap output bytes=683\n",
      "\t\tMap output materialized bytes=681\n",
      "\t\tInput split bytes=129\n",
      "\t\tCombine input records=62\n",
      "\t\tCombine output records=49\n",
      "\t\tSpilled Records=49\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=302514176\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=439\n",
      "2018-08-27 16:25:13,651 INFO mapred.LocalJobRunner: Finishing task: attempt_local560905394_0001_m_000001_0\n",
      "2018-08-27 16:25:13,651 INFO mapred.LocalJobRunner: Starting task: attempt_local560905394_0001_m_000002_0\n",
      "2018-08-27 16:25:13,652 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-08-27 16:25:13,652 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-08-27 16:25:13,652 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-08-27 16:25:13,652 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-08-27 16:25:13,653 INFO mapred.MapTask: Processing split: file:/Volumes/JetDrive/GitHub/big-data-analytics/input/text1.txt:0+351\n",
      "2018-08-27 16:25:13,711 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-08-27 16:25:13,711 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-08-27 16:25:13,711 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-08-27 16:25:13,711 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-08-27 16:25:13,711 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-08-27 16:25:13,712 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-08-27 16:25:13,713 INFO mapred.LocalJobRunner: \n",
      "2018-08-27 16:25:13,713 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-08-27 16:25:13,713 INFO mapred.MapTask: Spilling map output\n",
      "2018-08-27 16:25:13,713 INFO mapred.MapTask: bufstart = 0; bufend = 577; bufvoid = 104857600\n",
      "2018-08-27 16:25:13,713 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214172(104856688); length = 225/6553600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-27 16:25:13,766 INFO mapred.MapTask: Finished spill 0\n",
      "2018-08-27 16:25:13,773 INFO mapred.Task: Task:attempt_local560905394_0001_m_000002_0 is done. And is in the process of committing\n",
      "2018-08-27 16:25:13,774 INFO mapred.LocalJobRunner: map\n",
      "2018-08-27 16:25:13,774 INFO mapred.Task: Task 'attempt_local560905394_0001_m_000002_0' done.\n",
      "2018-08-27 16:25:13,774 INFO mapred.Task: Final Counters for attempt_local560905394_0001_m_000002_0: Counters: 18\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6246\n",
      "\t\tFILE: Number of bytes written=501463\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=57\n",
      "\t\tMap output bytes=577\n",
      "\t\tMap output materialized bytes=566\n",
      "\t\tInput split bytes=129\n",
      "\t\tCombine input records=57\n",
      "\t\tCombine output records=43\n",
      "\t\tSpilled Records=43\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=39\n",
      "\t\tTotal committed heap usage (bytes)=401604608\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=351\n",
      "2018-08-27 16:25:13,774 INFO mapred.LocalJobRunner: Finishing task: attempt_local560905394_0001_m_000002_0\n",
      "2018-08-27 16:25:13,774 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2018-08-27 16:25:13,777 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2018-08-27 16:25:13,777 INFO mapred.LocalJobRunner: Starting task: attempt_local560905394_0001_r_000000_0\n",
      "2018-08-27 16:25:13,782 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-08-27 16:25:13,783 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-08-27 16:25:13,783 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-08-27 16:25:13,783 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-08-27 16:25:13,785 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@480d6c06\n",
      "2018-08-27 16:25:13,786 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-08-27 16:25:13,799 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-08-27 16:25:13,801 INFO reduce.EventFetcher: attempt_local560905394_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-08-27 16:25:13,820 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local560905394_0001_m_000002_0 decomp: 562 len: 566 to MEMORY\n",
      "2018-08-27 16:25:13,825 INFO reduce.InMemoryMapOutput: Read 562 bytes from map-output for attempt_local560905394_0001_m_000002_0\n",
      "2018-08-27 16:25:13,826 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 562, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->562\n",
      "2018-08-27 16:25:13,827 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local560905394_0001_m_000001_0 decomp: 677 len: 681 to MEMORY\n",
      "2018-08-27 16:25:13,828 INFO reduce.InMemoryMapOutput: Read 677 bytes from map-output for attempt_local560905394_0001_m_000001_0\n",
      "2018-08-27 16:25:13,828 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 677, inMemoryMapOutputs.size() -> 2, commitMemory -> 562, usedMemory ->1239\n",
      "2018-08-27 16:25:13,829 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local560905394_0001_m_000000_0 decomp: 1370 len: 1374 to MEMORY\n",
      "2018-08-27 16:25:13,829 INFO reduce.InMemoryMapOutput: Read 1370 bytes from map-output for attempt_local560905394_0001_m_000000_0\n",
      "2018-08-27 16:25:13,829 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1370, inMemoryMapOutputs.size() -> 3, commitMemory -> 1239, usedMemory ->2609\n",
      "2018-08-27 16:25:13,829 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2018-08-27 16:25:13,830 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-08-27 16:25:13,830 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-08-27 16:25:13,839 INFO mapred.Merger: Merging 3 sorted segments\n",
      "2018-08-27 16:25:13,839 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 2583 bytes\n",
      "2018-08-27 16:25:13,846 INFO reduce.MergeManagerImpl: Merged 3 segments, 2609 bytes to disk to satisfy reduce memory limit\n",
      "2018-08-27 16:25:13,847 INFO reduce.MergeManagerImpl: Merging 1 files, 2609 bytes from disk\n",
      "2018-08-27 16:25:13,847 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-08-27 16:25:13,848 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-08-27 16:25:13,848 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2598 bytes\n",
      "2018-08-27 16:25:13,848 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-08-27 16:25:13,868 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-08-27 16:25:13,874 INFO mapred.Task: Task:attempt_local560905394_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-08-27 16:25:13,874 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-08-27 16:25:13,874 INFO mapred.Task: Task attempt_local560905394_0001_r_000000_0 is allowed to commit now\n",
      "2018-08-27 16:25:13,876 INFO output.FileOutputCommitter: Saved output of task 'attempt_local560905394_0001_r_000000_0' to file:/Volumes/JetDrive/GitHub/big-data-analytics/output\n",
      "2018-08-27 16:25:13,876 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "2018-08-27 16:25:13,876 INFO mapred.Task: Task 'attempt_local560905394_0001_r_000000_0' done.\n",
      "2018-08-27 16:25:13,876 INFO mapred.Task: Final Counters for attempt_local560905394_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11572\n",
      "\t\tFILE: Number of bytes written=505745\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=2621\n",
      "\t\tReduce input records=187\n",
      "\t\tReduce output records=159\n",
      "\t\tSpilled Records=187\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=401604608\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1673\n",
      "2018-08-27 16:25:13,876 INFO mapred.LocalJobRunner: Finishing task: attempt_local560905394_0001_r_000000_0\n",
      "2018-08-27 16:25:13,876 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2018-08-27 16:25:14,384 INFO mapreduce.Job: Job job_local560905394_0001 running in uber mode : false\n",
      "2018-08-27 16:25:14,385 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2018-08-27 16:25:14,386 INFO mapreduce.Job: Job job_local560905394_0001 completed successfully\n",
      "2018-08-27 16:25:14,394 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=27951\n",
      "\t\tFILE: Number of bytes written=2008225\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24\n",
      "\t\tMap output records=252\n",
      "\t\tMap output bytes=2873\n",
      "\t\tMap output materialized bytes=2621\n",
      "\t\tInput split bytes=387\n",
      "\t\tCombine input records=252\n",
      "\t\tCombine output records=187\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=2621\n",
      "\t\tReduce input records=187\n",
      "\t\tReduce output records=159\n",
      "\t\tSpilled Records=374\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=39\n",
      "\t\tTotal committed heap usage (bytes)=1302855680\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1882\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1673\n"
     ]
    }
   ],
   "source": [
    "## compila el programa\n",
    "!hadoop com.sun.tools.javac.Main WordCount.java\n",
    "\n",
    "## genera el archivo *.jar para ejecutarlo en hadoop\n",
    "!jar cf wc.jar WordCount*.class\n",
    "\n",
    "## ejecuta el proceso para los archivos \n",
    "## de texto en el directorio input\n",
    "!hadoop jar wc.jar WordCount input output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS     part-r-00000\r\n"
     ]
    }
   ],
   "source": [
    "## contenido el directorio de salida\n",
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DA)\t1\r\n",
      "(see\t1\r\n",
      "Analytics\t2\r\n",
      "Analytics,\t1\r\n",
      "Big\t1\r\n",
      "Data\t3\r\n",
      "Especially\t1\r\n",
      "Organizations\t1\r\n",
      "Since\t1\r\n",
      "Specifically,\t1\r\n",
      "The\t2\r\n",
      "a\t1\r\n",
      "about\t1\r\n",
      "aid\t1\r\n",
      "algorithms\t1\r\n",
      "analysis,\t1\r\n",
      "analysis.\t1\r\n",
      "analytics\t8\r\n",
      "analytics,\t8\r\n",
      "analytics.\t1\r\n",
      "analyze\t1\r\n",
      "and\t15\r\n",
      "application\t1\r\n",
      "apply\t1\r\n",
      "are\t1\r\n",
      "areas\t2\r\n",
      "assortment\t1\r\n",
      "be\t1\r\n",
      "big\t1\r\n",
      "business\t4\r\n",
      "by\t2\r\n",
      "call\t1\r\n",
      "can\t2\r\n",
      "certain\t1\r\n",
      "changes.\t1\r\n",
      "cognitive\t1\r\n",
      "commercial\t1\r\n",
      "communication\t1\r\n",
      "computation\t1\r\n",
      "computer\t2\r\n",
      "conclusions\t1\r\n",
      "contain,\t1\r\n",
      "credit\t1\r\n",
      "current\t1\r\n",
      "data\t4\r\n",
      "data),\t1\r\n",
      "data.\t1\r\n",
      "decision\t1\r\n",
      "decisions\t2\r\n",
      "describe,\t1\r\n",
      "descriptive\t1\r\n",
      "discovery,\t1\r\n",
      "disprove\t1\r\n",
      "draw\t1\r\n",
      "effects\t1\r\n",
      "enable\t1\r\n",
      "enterprise\t1\r\n",
      "evaluate\t1\r\n",
      "events,\t1\r\n",
      "examining\t1\r\n",
      "extensive\t1\r\n",
      "field\t1\r\n",
      "for\t1\r\n",
      "force\t1\r\n",
      "fraud\t1\r\n",
      "gaining\t1\r\n",
      "given\t1\r\n",
      "goal\t1\r\n",
      "harness\t1\r\n",
      "historical\t1\r\n",
      "hypotheses.\t1\r\n",
      "improve\t2\r\n",
      "improvements\t1\r\n",
      "in\t5\r\n",
      "include\t1\r\n",
      "increasingly\t1\r\n",
      "industries\t1\r\n",
      "information\t1\r\n",
      "information,\t1\r\n",
      "interpretation,\t1\r\n",
      "involves\t1\r\n",
      "is\t3\r\n",
      "knowledge\t1\r\n",
      "make\t2\r\n",
      "management,\t1\r\n",
      "marketing\t2\r\n",
      "mathematics.\t1\r\n",
      "may\t1\r\n",
      "meaningful\t1\r\n",
      "methods\t1\r\n",
      "mix\t1\r\n",
      "modeling,\t2\r\n",
      "models,\t1\r\n",
      "more-informed\t1\r\n",
      "most\t1\r\n",
      "of\t8\r\n",
      "often\t1\r\n",
      "on\t1\r\n",
      "operations\t1\r\n",
      "optimization\t1\r\n",
      "optimization,\t2\r\n",
      "or\t5\r\n",
      "order\t1\r\n",
      "organizations\t1\r\n",
      "past\t1\r\n",
      "patterns\t1\r\n",
      "performance\t1\r\n",
      "performance.\t2\r\n",
      "potential\t1\r\n",
      "predict,\t1\r\n",
      "predictive\t2\r\n",
      "prescriptive\t1\r\n",
      "price\t1\r\n",
      "process\t1\r\n",
      "programming\t1\r\n",
      "promotion\t1\r\n",
      "quantify\t1\r\n",
      "recorded\t1\r\n",
      "relies\t1\r\n",
      "require\t1\r\n",
      "research\t2\r\n",
      "researchers\t1\r\n",
      "retail\t1\r\n",
      "rich\t1\r\n",
      "risk\t1\r\n",
      "sales\t1\r\n",
      "scenario.\t1\r\n",
      "science,\t2\r\n",
      "scientific\t1\r\n",
      "scientists\t1\r\n",
      "sets\t1\r\n",
      "simultaneous\t1\r\n",
      "sizing\t1\r\n",
      "software\t1\r\n",
      "software.\t1\r\n",
      "specialized\t1\r\n",
      "speech\t1\r\n",
      "statistics,\t2\r\n",
      "stock-keeping\t1\r\n",
      "store\t1\r\n",
      "studying\t1\r\n",
      "systems\t1\r\n",
      "techniques\t1\r\n",
      "technologies\t1\r\n",
      "the\t10\r\n",
      "theories\t1\r\n",
      "they\t1\r\n",
      "to\t12\r\n",
      "tool\t1\r\n",
      "trends,\t1\r\n",
      "unit\t1\r\n",
      "used\t3\r\n",
      "valuable\t1\r\n",
      "verify\t1\r\n",
      "web\t1\r\n",
      "which\t1\r\n",
      "widely\t1\r\n",
      "with\t2\r\n",
      "within\t1\r\n"
     ]
    }
   ],
   "source": [
    "## imprime el resultado en pantalla\n",
    "!cat output/part-r-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Ejercicio.--** Cómo podría mejorar el código anterior? realice una implementación.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: WordCount*.*: No such file or directory\r\n",
      "rm: *.jar: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "## se limpia el directoroio de trabajo\n",
    "!rm WordCount*.* *.jar\n",
    "!rm -rf input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop/MapReduce -- WordCount en Java\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/big-data-analytics/tree/master/) para acceder al repositorio online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/big-data-analytics/tree/master/) para explorar el repositorio usando `nbviewer`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
