{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop/MapReduce -- WordCount en Java (Modo Standalone)\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/apache-hadoop-course/tree/master/) para acceder al repositorio online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/apache-hadoop-course/tree/master/) para explorar el repositorio usando `nbviewer`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del problema\n",
    "\n",
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos. Debido a los requerimientos de diseño (gran volúmen de datos y tiempos rápidos de respuesta) se desea implementar una arquitectura Big Data. **Se desea probar el código en la máquina local. El código debe ser escrito en Java.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepara el directorio de trabajo\n",
    "!rm -rf input output\n",
    "!mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tutorial detallado se encuentra en http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se planteará la solución en Java. Para construir una aplicación que usa MapReduce, el usuario debe implementar la función map y la función reduce; el sistema se encarga por si solo de ejecutarlas en el cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1.\n",
    "\n",
    "Se implementa el algoritmo de conteo de palabras y se guarda en el archivo `WordCount.java`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount.java\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.java\n",
    "\n",
    "import java.io.IOException;\n",
    "\n",
    "/*\n",
    " * Esta clase permite separar una frase (texto)\n",
    " * en las palabras que lo conforman. La lista\n",
    " * resultante puede ser iterada en un ciclo for\n",
    " */\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "/*\n",
    " *\n",
    " * Librerias requeridas para ejecutar Hadoop\n",
    " *\n",
    " */\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "/*\n",
    " * Esta clase implementa el mapper y el reducer\n",
    " */\n",
    "public class WordCount {\n",
    "\n",
    "  public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "       \n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "\n",
    "    /* \n",
    "     * en esta variable se guarda cada palabra leida        \n",
    "     * del flujo de entrada\n",
    "     */     \n",
    "    private Text word = new Text();\n",
    "\n",
    "    /* \n",
    "     * Este es el mapper. Para cada palabra \n",
    "     * leída, emite el par <word, 1>\n",
    "     */\n",
    "    public void map(Object key,       // Clave\n",
    "                    Text value,       // La linea de texto\n",
    "                    Context context   // Aplicación que se esta ejecutando\n",
    "                    ) throws IOException, InterruptedException {\n",
    "                              \n",
    "      // Convierte la línea de texto en una lista de strings\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "                              \n",
    "      // Ejecuta el ciclo para cada palabra \n",
    "      // de la lista de strings\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        // obtiene la palabra\n",
    "        word.set(itr.nextToken());\n",
    "\n",
    "        // escribe la pareja <word, 1> \n",
    "        // al flujo de salida\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "           \n",
    "    // Clase para imprimir un entero al flujo de salida       \n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    // Esta función es llamada para reducir \n",
    "    // una lista de valores que tienen la misma clave\n",
    "    public void reduce(Text key,                      // clave\n",
    "                       Iterable<IntWritable> values,  // lista de valores\n",
    "                       Context context                // Aplicación que se esta ejecutando\n",
    "                       ) throws IOException, InterruptedException {\n",
    "        \n",
    "      // itera sobre la lista de valores, sumandolos\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "        \n",
    "      // escribe la pareja <word, valor> al flujo\n",
    "      // de salida\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "    \n",
    "  /*\n",
    "   * Se crea la aplicación en Hadoop y se ejecuta\n",
    "   */\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    \n",
    "    /*\n",
    "     * El job corresponde a la aplicacion\n",
    "     */\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "      \n",
    "    /*\n",
    "     * La clase que contiene el mapper y el reducer\n",
    "     */\n",
    "    job.setJarByClass(WordCount.class);\n",
    "      \n",
    "    /* \n",
    "     * Clase que implementa el mapper  \n",
    "     */ \n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "      \n",
    "    /*\n",
    "     * El combiner es un reducer que se coloca a la salida\n",
    "     * del mapper para agilizar el computo\n",
    "     */\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    \n",
    "    /*\n",
    "     * Clase que implementa el reducer\n",
    "     */\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "      \n",
    "    /*\n",
    "     * Salida\n",
    "     */\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    \n",
    "    /*\n",
    "     * Formatos de entrada y salida\n",
    "     */\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "   \n",
    "    // resultado de la ejecución.\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2\n",
    "Se realiza la compilación del programa. Para que el programa se ejecute correctamente, se debió definir la variable de entorno `$HADOOP_HOME`, la cual apunta al directorio donde se encuentra Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$HADOOP_HOME/bin/hadoop com.sun.tools.javac.Main WordCount.java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3\n",
    "\n",
    "Se genera el archivo de aplicación de Java, para luego ejecutarlo usando Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jar cf wc.jar WordCount*.class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo `wc.jar` debe aparecer en el directorio actua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc.jar\n"
     ]
    }
   ],
   "source": [
    "!ls *.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4\n",
    "\n",
    "En este tutorial se usará el modo *standalone*, en el que la máquina virtual de Java se ejecuta en un único hilo de procesamiento. Este modo es utilizado para depurar programas. En este modo, solo se requiere que no haya ninguna especificación en `<configuration>` ... `</configuration>` de los archivos `etc/hadoop/core-site.xml`  y `etc/hadoop/hdfs-site.xml` que se encuentran en el directorio de instalación de Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "\n",
      "<!-- Put site-specific property overrides in this file. -->\n",
      "\n",
      "<configuration>\n",
      "    <property>\n",
      "        <name>fs.defaultFS</name>\n",
      "        <value>hdfs://localhost:9000</value>\n",
      "    </property>\n",
      "</configuration>"
     ]
    }
   ],
   "source": [
    "## Se visualiza el contenido actual de los archivos\n",
    "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "\n",
      "<!-- Put site-specific property overrides in this file. -->\n",
      "\n",
      " \n",
      "<configuration>\n",
      "    <property>\n",
      "        <name>dfs.replication</name>\n",
      "        <value>1</value>\n",
      "    </property>\n",
      "</configuration>"
     ]
    }
   ],
   "source": [
    "## Se visualiza el contenido actual de los archivos\n",
    "!cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Por facilidad, este directorio contiene una copia del archivo\n",
    "## la cual se copiará al directorio de Hadoop para evitar errores\n",
    "## en la ejecución del programa\n",
    "!cp core-site.xml.standalone  $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "!cp hdfs-site.xml.standalone  $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5\n",
    "\n",
    "Se eejecuta el programa para realizar el conteo de palabras. En el modo *standalone*, los directorios de entrada (`input/`) y salida (`output/`) se encuentran en la carpeta actual donde se abrio Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 19:43:34,351 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-09-02 19:43:34,741 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "2018-09-02 19:43:34,983 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2018-09-02 19:43:34,983 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2018-09-02 19:43:35,167 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "2018-09-02 19:43:35,261 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2018-09-02 19:43:35,316 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2018-09-02 19:43:35,461 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local709984956_0001\n",
      "2018-09-02 19:43:35,462 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2018-09-02 19:43:35,558 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2018-09-02 19:43:35,559 INFO mapreduce.Job: Running job: job_local709984956_0001\n",
      "2018-09-02 19:43:35,560 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2018-09-02 19:43:35,565 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 19:43:35,566 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 19:43:35,566 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2018-09-02 19:43:35,604 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2018-09-02 19:43:35,605 INFO mapred.LocalJobRunner: Starting task: attempt_local709984956_0001_m_000000_0\n",
      "2018-09-02 19:43:35,621 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 19:43:35,621 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 19:43:35,628 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 19:43:35,628 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 19:43:35,632 INFO mapred.MapTask: Processing split: file:/Volumes/JetDrive/GitHub/apache-hadoop-course/input/text0.txt:0+1092\n",
      "2018-09-02 19:43:35,697 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-09-02 19:43:35,697 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-09-02 19:43:35,697 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-09-02 19:43:35,697 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-09-02 19:43:35,697 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-09-02 19:43:35,700 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-09-02 19:43:35,707 INFO mapred.LocalJobRunner: \n",
      "2018-09-02 19:43:35,708 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-09-02 19:43:35,708 INFO mapred.MapTask: Spilling map output\n",
      "2018-09-02 19:43:35,708 INFO mapred.MapTask: bufstart = 0; bufend = 1613; bufvoid = 104857600\n",
      "2018-09-02 19:43:35,708 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213868(104855472); length = 529/6553600\n",
      "2018-09-02 19:43:35,735 INFO mapred.MapTask: Finished spill 0\n",
      "2018-09-02 19:43:35,748 INFO mapred.Task: Task:attempt_local709984956_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-09-02 19:43:35,749 INFO mapred.LocalJobRunner: map\n",
      "2018-09-02 19:43:35,749 INFO mapred.Task: Task 'attempt_local709984956_0001_m_000000_0' done.\n",
      "2018-09-02 19:43:35,755 INFO mapred.Task: Final Counters for attempt_local709984956_0001_m_000000_0: Counters: 18\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4659\n",
      "\t\tFILE: Number of bytes written=500173\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=14\n",
      "\t\tMap output records=133\n",
      "\t\tMap output bytes=1613\n",
      "\t\tMap output materialized bytes=1374\n",
      "\t\tInput split bytes=131\n",
      "\t\tCombine input records=133\n",
      "\t\tCombine output records=95\n",
      "\t\tSpilled Records=95\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=197656576\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1092\n",
      "2018-09-02 19:43:35,755 INFO mapred.LocalJobRunner: Finishing task: attempt_local709984956_0001_m_000000_0\n",
      "2018-09-02 19:43:35,756 INFO mapred.LocalJobRunner: Starting task: attempt_local709984956_0001_m_000001_0\n",
      "2018-09-02 19:43:35,756 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 19:43:35,757 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 19:43:35,757 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 19:43:35,757 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 19:43:35,758 INFO mapred.MapTask: Processing split: file:/Volumes/JetDrive/GitHub/apache-hadoop-course/input/text2.txt:0+439\n",
      "2018-09-02 19:43:35,812 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-09-02 19:43:35,812 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-09-02 19:43:35,812 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-09-02 19:43:35,812 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-09-02 19:43:35,812 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-09-02 19:43:35,813 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-09-02 19:43:35,815 INFO mapred.LocalJobRunner: \n",
      "2018-09-02 19:43:35,815 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-09-02 19:43:35,815 INFO mapred.MapTask: Spilling map output\n",
      "2018-09-02 19:43:35,815 INFO mapred.MapTask: bufstart = 0; bufend = 683; bufvoid = 104857600\n",
      "2018-09-02 19:43:35,815 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600\n",
      "2018-09-02 19:43:35,827 INFO mapred.MapTask: Finished spill 0\n",
      "2018-09-02 19:43:35,833 INFO mapred.Task: Task:attempt_local709984956_0001_m_000001_0 is done. And is in the process of committing\n",
      "2018-09-02 19:43:35,834 INFO mapred.LocalJobRunner: map\n",
      "2018-09-02 19:43:35,834 INFO mapred.Task: Task 'attempt_local709984956_0001_m_000001_0' done.\n",
      "2018-09-02 19:43:35,835 INFO mapred.Task: Final Counters for attempt_local709984956_0001_m_000001_0: Counters: 18\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5510\n",
      "\t\tFILE: Number of bytes written=500886\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=62\n",
      "\t\tMap output bytes=683\n",
      "\t\tMap output materialized bytes=681\n",
      "\t\tInput split bytes=131\n",
      "\t\tCombine input records=62\n",
      "\t\tCombine output records=49\n",
      "\t\tSpilled Records=49\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=303038464\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=439\n",
      "2018-09-02 19:43:35,835 INFO mapred.LocalJobRunner: Finishing task: attempt_local709984956_0001_m_000001_0\n",
      "2018-09-02 19:43:35,835 INFO mapred.LocalJobRunner: Starting task: attempt_local709984956_0001_m_000002_0\n",
      "2018-09-02 19:43:35,835 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 19:43:35,835 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 19:43:35,836 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 19:43:35,836 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 19:43:35,837 INFO mapred.MapTask: Processing split: file:/Volumes/JetDrive/GitHub/apache-hadoop-course/input/text1.txt:0+351\n",
      "2018-09-02 19:43:35,891 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-09-02 19:43:35,891 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-09-02 19:43:35,891 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-09-02 19:43:35,891 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-09-02 19:43:35,891 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-09-02 19:43:35,930 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-09-02 19:43:35,931 INFO mapred.LocalJobRunner: \n",
      "2018-09-02 19:43:35,931 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-09-02 19:43:35,931 INFO mapred.MapTask: Spilling map output\n",
      "2018-09-02 19:43:35,931 INFO mapred.MapTask: bufstart = 0; bufend = 577; bufvoid = 104857600\n",
      "2018-09-02 19:43:35,931 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214172(104856688); length = 225/6553600\n",
      "2018-09-02 19:43:35,944 INFO mapred.MapTask: Finished spill 0\n",
      "2018-09-02 19:43:35,950 INFO mapred.Task: Task:attempt_local709984956_0001_m_000002_0 is done. And is in the process of committing\n",
      "2018-09-02 19:43:35,951 INFO mapred.LocalJobRunner: map\n",
      "2018-09-02 19:43:35,951 INFO mapred.Task: Task 'attempt_local709984956_0001_m_000002_0' done.\n",
      "2018-09-02 19:43:35,951 INFO mapred.Task: Final Counters for attempt_local709984956_0001_m_000002_0: Counters: 18\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6273\n",
      "\t\tFILE: Number of bytes written=501484\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=57\n",
      "\t\tMap output bytes=577\n",
      "\t\tMap output materialized bytes=566\n",
      "\t\tInput split bytes=131\n",
      "\t\tCombine input records=57\n",
      "\t\tCombine output records=43\n",
      "\t\tSpilled Records=43\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=38\n",
      "\t\tTotal committed heap usage (bytes)=402128896\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=351\n",
      "2018-09-02 19:43:35,951 INFO mapred.LocalJobRunner: Finishing task: attempt_local709984956_0001_m_000002_0\n",
      "2018-09-02 19:43:35,952 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2018-09-02 19:43:35,953 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2018-09-02 19:43:35,954 INFO mapred.LocalJobRunner: Starting task: attempt_local709984956_0001_r_000000_0\n",
      "2018-09-02 19:43:35,958 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 19:43:35,958 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 19:43:35,959 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 19:43:35,959 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 19:43:35,961 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@52af128e\n",
      "2018-09-02 19:43:35,962 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-09-02 19:43:35,974 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-09-02 19:43:35,976 INFO reduce.EventFetcher: attempt_local709984956_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-09-02 19:43:35,995 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local709984956_0001_m_000001_0 decomp: 677 len: 681 to MEMORY\n",
      "2018-09-02 19:43:36,000 INFO reduce.InMemoryMapOutput: Read 677 bytes from map-output for attempt_local709984956_0001_m_000001_0\n",
      "2018-09-02 19:43:36,001 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 677, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->677\n",
      "2018-09-02 19:43:36,003 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local709984956_0001_m_000002_0 decomp: 562 len: 566 to MEMORY\n",
      "2018-09-02 19:43:36,003 INFO reduce.InMemoryMapOutput: Read 562 bytes from map-output for attempt_local709984956_0001_m_000002_0\n",
      "2018-09-02 19:43:36,003 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 562, inMemoryMapOutputs.size() -> 2, commitMemory -> 677, usedMemory ->1239\n",
      "2018-09-02 19:43:36,004 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local709984956_0001_m_000000_0 decomp: 1370 len: 1374 to MEMORY\n",
      "2018-09-02 19:43:36,004 INFO reduce.InMemoryMapOutput: Read 1370 bytes from map-output for attempt_local709984956_0001_m_000000_0\n",
      "2018-09-02 19:43:36,004 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1370, inMemoryMapOutputs.size() -> 3, commitMemory -> 1239, usedMemory ->2609\n",
      "2018-09-02 19:43:36,005 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2018-09-02 19:43:36,005 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-09-02 19:43:36,005 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-09-02 19:43:36,014 INFO mapred.Merger: Merging 3 sorted segments\n",
      "2018-09-02 19:43:36,015 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 2583 bytes\n",
      "2018-09-02 19:43:36,021 INFO reduce.MergeManagerImpl: Merged 3 segments, 2609 bytes to disk to satisfy reduce memory limit\n",
      "2018-09-02 19:43:36,022 INFO reduce.MergeManagerImpl: Merging 1 files, 2609 bytes from disk\n",
      "2018-09-02 19:43:36,022 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-09-02 19:43:36,022 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-09-02 19:43:36,022 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2598 bytes\n",
      "2018-09-02 19:43:36,023 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-09-02 19:43:36,044 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-09-02 19:43:36,050 INFO mapred.Task: Task:attempt_local709984956_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-09-02 19:43:36,051 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-09-02 19:43:36,051 INFO mapred.Task: Task attempt_local709984956_0001_r_000000_0 is allowed to commit now\n",
      "2018-09-02 19:43:36,052 INFO output.FileOutputCommitter: Saved output of task 'attempt_local709984956_0001_r_000000_0' to file:/Volumes/JetDrive/GitHub/apache-hadoop-course/output\n",
      "2018-09-02 19:43:36,053 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "2018-09-02 19:43:36,053 INFO mapred.Task: Task 'attempt_local709984956_0001_r_000000_0' done.\n",
      "2018-09-02 19:43:36,053 INFO mapred.Task: Final Counters for attempt_local709984956_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11599\n",
      "\t\tFILE: Number of bytes written=505766\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=2621\n",
      "\t\tReduce input records=187\n",
      "\t\tReduce output records=159\n",
      "\t\tSpilled Records=187\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=402128896\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1673\n",
      "2018-09-02 19:43:36,053 INFO mapred.LocalJobRunner: Finishing task: attempt_local709984956_0001_r_000000_0\n",
      "2018-09-02 19:43:36,053 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2018-09-02 19:43:36,562 INFO mapreduce.Job: Job job_local709984956_0001 running in uber mode : false\n",
      "2018-09-02 19:43:36,563 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2018-09-02 19:43:36,564 INFO mapreduce.Job: Job job_local709984956_0001 completed successfully\n",
      "2018-09-02 19:43:36,570 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=28041\n",
      "\t\tFILE: Number of bytes written=2008309\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24\n",
      "\t\tMap output records=252\n",
      "\t\tMap output bytes=2873\n",
      "\t\tMap output materialized bytes=2621\n",
      "\t\tInput split bytes=393\n",
      "\t\tCombine input records=252\n",
      "\t\tCombine output records=187\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=2621\n",
      "\t\tReduce input records=187\n",
      "\t\tReduce output records=159\n",
      "\t\tSpilled Records=374\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=38\n",
      "\t\tTotal committed heap usage (bytes)=1304952832\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1882\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1673\n"
     ]
    }
   ],
   "source": [
    "## ejecuta el proceso para los archivos \n",
    "## de texto en el directorio input\n",
    "!$HADOOP_HOME/bin/hadoop jar wc.jar WordCount input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 6\n",
    "La salida del paso anterior se revisa para determinar si hay algún mensaje de error. Si el proceso se ejecutó correctamente, el directorio `ouput/` contiene el resultado de la ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS     part-r-00000\n"
     ]
    }
   ],
   "source": [
    "## contenido el directorio de salida\n",
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Specifically,\t1\n"
     ]
    }
   ],
   "source": [
    "## imprime el resultado en pantalla\n",
    "!cat output/part-r-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Ejercicio.--** Cómo se puede ejecutar el conteo de palabras únicamente para el archivo `text0.txt`?-\n",
    "\n",
    "**Ejercicio.--** Cómo podría mejorar el código anterior? realice una implementación.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se limpia el directoroio de trabajo\n",
    "!rm WordCount*.* *.jar\n",
    "!rm -rf input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop/MapReduce -- WordCount en Java (Modo Standalone)\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/apache-hadoop-course/tree/master/) para acceder al repositorio online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/apache-hadoop-course/tree/master/) para explorar el repositorio usando `nbviewer`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
