{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop/MapReduce -- WordCount en Java (Modo Pseudo)\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/apache-hadoop-course/tree/master/) para acceder al repositorio online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/apache-hadoop-course/tree/master/) para explorar el repositorio usando `nbviewer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del problema\n",
    "\n",
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos. Debido a los requerimientos de diseño (gran volúmen de datos y tiempos rápidos de respuesta) se desea implementar una arquitectura Big Data. **El código debe ser escrito en Java y correrse en el ámbiente distribuido.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparación del directorio de trabajo\n",
    "!rm -rf input output\n",
    "!mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se utiliza la misma implementación del tutorial anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1.\n",
    "\n",
    "**Este paso es idéntico en el tutorial anterior.** Se implementa el algoritmo de conteo de palabras y se guarda en el archivo `WordCount.java`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount.java\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.java\n",
    "\n",
    "import java.io.IOException;\n",
    "\n",
    "/*\n",
    " * Esta clase permite separar una frase (texto)\n",
    " * en las palabras que lo conforman. La lista\n",
    " * resultante puede ser iterada en un ciclo for\n",
    " */\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "/*\n",
    " *\n",
    " * Librerias requeridas para ejecutar Hadoop\n",
    " *\n",
    " */\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "/*\n",
    " * Esta clase implementa el mapper y el reducer\n",
    " */\n",
    "public class WordCount {\n",
    "\n",
    "  public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "       \n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "\n",
    "    /* \n",
    "     * en esta variable se guarda cada palabra leida        \n",
    "     * del flujo de entrada\n",
    "     */     \n",
    "    private Text word = new Text();\n",
    "\n",
    "    /* \n",
    "     * Este es el mapper. Para cada palabra \n",
    "     * leída, emite el par <word, 1>\n",
    "     */\n",
    "    public void map(Object key,       // Clave\n",
    "                    Text value,       // La linea de texto\n",
    "                    Context context   // Aplicación que se esta ejecutando\n",
    "                    ) throws IOException, InterruptedException {\n",
    "                              \n",
    "      // Convierte la línea de texto en una lista de strings\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "                              \n",
    "      // Ejecuta el ciclo para cada palabra \n",
    "      // de la lista de strings\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        // obtiene la palabra\n",
    "        word.set(itr.nextToken());\n",
    "\n",
    "        // escribe la pareja <word, 1> \n",
    "        // al flujo de salida\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "           \n",
    "    // Clase para imprimir un entero al flujo de salida       \n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    // Esta función es llamada para reducir \n",
    "    // una lista de valores que tienen la misma clave\n",
    "    public void reduce(Text key,                      // clave\n",
    "                       Iterable<IntWritable> values,  // lista de valores\n",
    "                       Context context                // Aplicación que se esta ejecutando\n",
    "                       ) throws IOException, InterruptedException {\n",
    "        \n",
    "      // itera sobre la lista de valores, sumandolos\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "        \n",
    "      // escribe la pareja <word, valor> al flujo\n",
    "      // de salida\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "    \n",
    "  /*\n",
    "   * Se crea la aplicación en Hadoop y se ejecuta\n",
    "   */\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    \n",
    "    /*\n",
    "     * El job corresponde a la aplicacion\n",
    "     */\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "      \n",
    "    /*\n",
    "     * La clase que contiene el mapper y el reducer\n",
    "     */\n",
    "    job.setJarByClass(WordCount.class);\n",
    "      \n",
    "    /* \n",
    "     * Clase que implementa el mapper  \n",
    "     */ \n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "      \n",
    "    /*\n",
    "     * El combiner es un reducer que se coloca a la salida\n",
    "     * del mapper para agilizar el computo\n",
    "     */\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    \n",
    "    /*\n",
    "     * Clase que implementa el reducer\n",
    "     */\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "      \n",
    "    /*\n",
    "     * Salida\n",
    "     */\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    \n",
    "    /*\n",
    "     * Formatos de entrada y salida\n",
    "     */\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "   \n",
    "    // resultado de la ejecución.\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2\n",
    "\n",
    "**Este paso es idéntico en el tutorial anterior.** Se realiza la compilación del programa. Para que el programa se ejecute correctamente, se debió definir la variable de entorno `$HADOOP_HOME`, la cual apunta al directorio donde se encuentra Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compila el programa\n",
    "!$HADOOP_HOME/bin/hadoop com.sun.tools.javac.Main WordCount.java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3\n",
    "\n",
    "**Este paso es idéntico en el tutorial anterior.** Se genera el archivo de aplicación de Java, para luego ejecutarlo usando Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## genera el archivo *.jar para ejecutarlo en hadoop\n",
    "!jar cf wc.jar WordCount*.class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo `wc.jar` debe aparecer en el directorio actua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc.jar\n"
     ]
    }
   ],
   "source": [
    "!ls *.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4\n",
    "\n",
    "Se verifica que la instalación este configurada para correr en modo pseudo-distribuido, permitiendo varios hilos de procesamiento. Este modo es similar a la ejecución en un cluster. En este caso, el usuario debe verificar el contenido de los archivos `etc/hadoop/core-site.xml`  y `etc/hadoop/hdfs-site.xml` que se encuentran en el directorio de instalación de Hadoop. Véase http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Por facilidad, este directorio contiene una copia del archivo\n",
    "## la cual se copiará al directorio de Hadoop para evitar errores\n",
    "## en la ejecución del programa\n",
    "!cp core-site.xml.pseudo  $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "!cp hdfs-site.xml.pseudo  $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5\n",
    "\n",
    "Para ejecutar Hadoop en modo pseudo-distribuido, debe formatear el sistema de archivos (si aún no lo ha hecho). Abra el Terminal y ejecute\n",
    "\n",
    "    $HADOOP_HOME/bin/hdfs namenode -format\n",
    "    \n",
    "    \n",
    "Seguidamente, debe iniciar el servicio. Para ello, en la consola de comandos (no se puede realizar directamente desde Jupyter) ejecute:\n",
    "\n",
    "    $HADOOP_HOME/sbin/start-dfs.sh\n",
    "    \n",
    "Una vez se haya iniciado el servicio, verifique que la interface a NameNode funcione correctamente. Abra la dirección http://localhost:9870. La interfaz debe ser similar a la que aparece en la siguiente figura.\n",
    "\n",
    "![namenode](images/mac-namenode.png)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copie los archivos de entrada al HDFS para que puedan ser ejecutados por Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gestión de archivos entre el sistema local y el HDFS se realiza mediante comandos similares a los del sistema operativo Unix en Terminal. A continuación se resumen los principales comandos.\n",
    "\n",
    "* `hadoop fs -help`:  Imprime la ayuda en pantalla para todos los comandos.\n",
    "\n",
    "\n",
    "**Gestion de directorios y archivos.**\n",
    "\n",
    "\n",
    "* `hadoop fs -ls <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -mkdir <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -rmdir <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -cp <src> <dest>`\n",
    "\n",
    "\n",
    "* `hadoop fs -mv <src> <dest>`\n",
    "\n",
    "\n",
    "* `hadoop fs -rm <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -cat <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -head <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -tail <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -text <path>`. Imprime el arachivo en `<path>` y lo imprime en formato texto. Soporta archivos zip, TextRecordInputStream y Avro.\n",
    "\n",
    "\n",
    "* `hadoop fs -stat <path>`: Imprime estadísticos de `<path>`.\n",
    "\n",
    "\n",
    "**Transferencia de información entre el sistema local y el HDFS**.\n",
    "\n",
    "\n",
    "* `hadoop fs -get <src> <localdest>` / `hadoop fs -copyToLocal <src> <localdest>`. Copia el contenido de `<src>` en el HDFS en `<localdest>` en el sistema local.\n",
    "\n",
    "\n",
    "* `hadoop fs -put <localsrc> <dest>` / `hadoop fs -copyFromLocal <src> <localdest>`. Copia el contenido de `<localsrc>` en el sistema local a `<dest>` en el HDFS.\n",
    "\n",
    "\n",
    "* `hadoop fs -count <path>`. Cuenta el número de directorios, archivos y bytes en `<path>`.\n",
    "\n",
    "\n",
    "* `hadoop fs -appendToFile <localsrc> <dest>`: pega al final de `<dest>` el contenido de los archivos en `<localsrc>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:18:28,471 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## crea el directorio para los archivos de entrada\n",
    "!$HADOOP_HOME/bin/hadoop fs -mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:18:32,633 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## Copia el contenido de la carpeta input a la carpeta input en el HDFS\n",
    "!$HADOOP_HOME/bin/hadoop fs -put  input/* input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:18:35,602 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jdvelasq supergroup       1092 2018-09-02 20:18 input/text0.txt\n",
      "-rw-r--r--   1 jdvelasq supergroup        351 2018-09-02 20:18 input/text1.txt\n",
      "-rw-r--r--   1 jdvelasq supergroup        439 2018-09-02 20:18 input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "## verifica el contenido del directorio\n",
    "!$HADOOP_HOME/bin/hadoop fs -ls input/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 7\n",
    "\n",
    "Ejecuta el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:18:40,541 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-09-02 20:18:41,304 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "2018-09-02 20:18:41,504 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2018-09-02 20:18:41,504 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2018-09-02 20:18:41,771 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "2018-09-02 20:18:41,881 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2018-09-02 20:18:41,937 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2018-09-02 20:18:42,050 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local829939055_0001\n",
      "2018-09-02 20:18:42,052 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2018-09-02 20:18:42,156 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2018-09-02 20:18:42,156 INFO mapreduce.Job: Running job: job_local829939055_0001\n",
      "2018-09-02 20:18:42,157 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2018-09-02 20:18:42,163 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 20:18:42,163 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 20:18:42,163 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2018-09-02 20:18:42,189 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2018-09-02 20:18:42,189 INFO mapred.LocalJobRunner: Starting task: attempt_local829939055_0001_m_000000_0\n",
      "2018-09-02 20:18:42,207 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 20:18:42,207 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 20:18:42,215 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 20:18:42,216 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 20:18:42,219 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jdvelasq/input/text0.txt:0+1092\n",
      "2018-09-02 20:18:42,287 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-09-02 20:18:42,288 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-09-02 20:18:42,288 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-09-02 20:18:42,288 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-09-02 20:18:42,288 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-09-02 20:18:42,291 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-09-02 20:18:42,363 INFO mapred.LocalJobRunner: \n",
      "2018-09-02 20:18:42,365 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-09-02 20:18:42,365 INFO mapred.MapTask: Spilling map output\n",
      "2018-09-02 20:18:42,365 INFO mapred.MapTask: bufstart = 0; bufend = 1613; bufvoid = 104857600\n",
      "2018-09-02 20:18:42,365 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213868(104855472); length = 529/6553600\n",
      "2018-09-02 20:18:42,392 INFO mapred.MapTask: Finished spill 0\n",
      "2018-09-02 20:18:42,404 INFO mapred.Task: Task:attempt_local829939055_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-09-02 20:18:42,407 INFO mapred.LocalJobRunner: map\n",
      "2018-09-02 20:18:42,407 INFO mapred.Task: Task 'attempt_local829939055_0001_m_000000_0' done.\n",
      "2018-09-02 20:18:42,413 INFO mapred.Task: Final Counters for attempt_local829939055_0001_m_000000_0: Counters: 23\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3529\n",
      "\t\tFILE: Number of bytes written=500588\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1092\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=5\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=14\n",
      "\t\tMap output records=133\n",
      "\t\tMap output bytes=1613\n",
      "\t\tMap output materialized bytes=1374\n",
      "\t\tInput split bytes=116\n",
      "\t\tCombine input records=133\n",
      "\t\tCombine output records=95\n",
      "\t\tSpilled Records=95\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=318767104\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1092\n",
      "2018-09-02 20:18:42,413 INFO mapred.LocalJobRunner: Finishing task: attempt_local829939055_0001_m_000000_0\n",
      "2018-09-02 20:18:42,414 INFO mapred.LocalJobRunner: Starting task: attempt_local829939055_0001_m_000001_0\n",
      "2018-09-02 20:18:42,415 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 20:18:42,415 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 20:18:42,416 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 20:18:42,416 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 20:18:42,417 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jdvelasq/input/text2.txt:0+439\n",
      "2018-09-02 20:18:42,474 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-09-02 20:18:42,474 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-09-02 20:18:42,474 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-09-02 20:18:42,474 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-09-02 20:18:42,474 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-09-02 20:18:42,475 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-09-02 20:18:42,479 INFO mapred.LocalJobRunner: \n",
      "2018-09-02 20:18:42,479 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-09-02 20:18:42,479 INFO mapred.MapTask: Spilling map output\n",
      "2018-09-02 20:18:42,479 INFO mapred.MapTask: bufstart = 0; bufend = 683; bufvoid = 104857600\n",
      "2018-09-02 20:18:42,480 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600\n",
      "2018-09-02 20:18:42,492 INFO mapred.MapTask: Finished spill 0\n",
      "2018-09-02 20:18:42,499 INFO mapred.Task: Task:attempt_local829939055_0001_m_000001_0 is done. And is in the process of committing\n",
      "2018-09-02 20:18:42,500 INFO mapred.LocalJobRunner: map\n",
      "2018-09-02 20:18:42,500 INFO mapred.Task: Task 'attempt_local829939055_0001_m_000001_0' done.\n",
      "2018-09-02 20:18:42,501 INFO mapred.Task: Final Counters for attempt_local829939055_0001_m_000001_0: Counters: 23\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3896\n",
      "\t\tFILE: Number of bytes written=501301\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1531\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=7\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=62\n",
      "\t\tMap output bytes=683\n",
      "\t\tMap output materialized bytes=681\n",
      "\t\tInput split bytes=116\n",
      "\t\tCombine input records=62\n",
      "\t\tCombine output records=49\n",
      "\t\tSpilled Records=49\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=424148992\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=439\n",
      "2018-09-02 20:18:42,501 INFO mapred.LocalJobRunner: Finishing task: attempt_local829939055_0001_m_000001_0\n",
      "2018-09-02 20:18:42,501 INFO mapred.LocalJobRunner: Starting task: attempt_local829939055_0001_m_000002_0\n",
      "2018-09-02 20:18:42,502 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 20:18:42,502 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 20:18:42,502 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 20:18:42,502 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 20:18:42,503 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jdvelasq/input/text1.txt:0+351\n",
      "2018-09-02 20:18:42,559 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-09-02 20:18:42,560 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-09-02 20:18:42,560 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-09-02 20:18:42,560 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-09-02 20:18:42,560 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-09-02 20:18:42,560 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-09-02 20:18:42,565 INFO mapred.LocalJobRunner: \n",
      "2018-09-02 20:18:42,565 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-09-02 20:18:42,565 INFO mapred.MapTask: Spilling map output\n",
      "2018-09-02 20:18:42,565 INFO mapred.MapTask: bufstart = 0; bufend = 577; bufvoid = 104857600\n",
      "2018-09-02 20:18:42,565 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214172(104856688); length = 225/6553600\n",
      "2018-09-02 20:18:42,577 INFO mapred.MapTask: Finished spill 0\n",
      "2018-09-02 20:18:42,583 INFO mapred.Task: Task:attempt_local829939055_0001_m_000002_0 is done. And is in the process of committing\n",
      "2018-09-02 20:18:42,585 INFO mapred.LocalJobRunner: map\n",
      "2018-09-02 20:18:42,585 INFO mapred.Task: Task 'attempt_local829939055_0001_m_000002_0' done.\n",
      "2018-09-02 20:18:42,586 INFO mapred.Task: Final Counters for attempt_local829939055_0001_m_000002_0: Counters: 23\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4263\n",
      "\t\tFILE: Number of bytes written=501899\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1882\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=57\n",
      "\t\tMap output bytes=577\n",
      "\t\tMap output materialized bytes=566\n",
      "\t\tInput split bytes=116\n",
      "\t\tCombine input records=57\n",
      "\t\tCombine output records=43\n",
      "\t\tSpilled Records=43\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=529530880\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=351\n",
      "2018-09-02 20:18:42,586 INFO mapred.LocalJobRunner: Finishing task: attempt_local829939055_0001_m_000002_0\n",
      "2018-09-02 20:18:42,586 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2018-09-02 20:18:42,588 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2018-09-02 20:18:42,588 INFO mapred.LocalJobRunner: Starting task: attempt_local829939055_0001_r_000000_0\n",
      "2018-09-02 20:18:42,593 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-09-02 20:18:42,593 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-09-02 20:18:42,594 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-09-02 20:18:42,594 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-09-02 20:18:42,596 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@35d41ccb\n",
      "2018-09-02 20:18:42,597 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-09-02 20:18:42,610 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-09-02 20:18:42,612 INFO reduce.EventFetcher: attempt_local829939055_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-09-02 20:18:42,629 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local829939055_0001_m_000002_0 decomp: 562 len: 566 to MEMORY\n",
      "2018-09-02 20:18:42,632 INFO reduce.InMemoryMapOutput: Read 562 bytes from map-output for attempt_local829939055_0001_m_000002_0\n",
      "2018-09-02 20:18:42,633 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 562, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->562\n",
      "2018-09-02 20:18:42,634 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local829939055_0001_m_000001_0 decomp: 677 len: 681 to MEMORY\n",
      "2018-09-02 20:18:42,635 INFO reduce.InMemoryMapOutput: Read 677 bytes from map-output for attempt_local829939055_0001_m_000001_0\n",
      "2018-09-02 20:18:42,635 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 677, inMemoryMapOutputs.size() -> 2, commitMemory -> 562, usedMemory ->1239\n",
      "2018-09-02 20:18:42,636 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local829939055_0001_m_000000_0 decomp: 1370 len: 1374 to MEMORY\n",
      "2018-09-02 20:18:42,636 INFO reduce.InMemoryMapOutput: Read 1370 bytes from map-output for attempt_local829939055_0001_m_000000_0\n",
      "2018-09-02 20:18:42,636 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1370, inMemoryMapOutputs.size() -> 3, commitMemory -> 1239, usedMemory ->2609\n",
      "2018-09-02 20:18:42,637 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2018-09-02 20:18:42,637 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-09-02 20:18:42,637 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-09-02 20:18:42,647 INFO mapred.Merger: Merging 3 sorted segments\n",
      "2018-09-02 20:18:42,647 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 2583 bytes\n",
      "2018-09-02 20:18:42,654 INFO reduce.MergeManagerImpl: Merged 3 segments, 2609 bytes to disk to satisfy reduce memory limit\n",
      "2018-09-02 20:18:42,654 INFO reduce.MergeManagerImpl: Merging 1 files, 2609 bytes from disk\n",
      "2018-09-02 20:18:42,655 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-09-02 20:18:42,655 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-09-02 20:18:42,655 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2598 bytes\n",
      "2018-09-02 20:18:42,655 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-09-02 20:18:42,678 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-09-02 20:18:42,719 INFO mapred.Task: Task:attempt_local829939055_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-09-02 20:18:42,721 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-09-02 20:18:42,721 INFO mapred.Task: Task attempt_local829939055_0001_r_000000_0 is allowed to commit now\n",
      "2018-09-02 20:18:42,729 INFO output.FileOutputCommitter: Saved output of task 'attempt_local829939055_0001_r_000000_0' to hdfs://localhost:9000/user/jdvelasq/output\n",
      "2018-09-02 20:18:42,730 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "2018-09-02 20:18:42,730 INFO mapred.Task: Task 'attempt_local829939055_0001_r_000000_0' done.\n",
      "2018-09-02 20:18:42,730 INFO mapred.Task: Final Counters for attempt_local829939055_0001_r_000000_0: Counters: 29\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9589\n",
      "\t\tFILE: Number of bytes written=504508\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1882\n",
      "\t\tHDFS: Number of bytes written=1649\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=2621\n",
      "\t\tReduce input records=187\n",
      "\t\tReduce output records=159\n",
      "\t\tSpilled Records=187\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=529530880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1649\n",
      "2018-09-02 20:18:42,731 INFO mapred.LocalJobRunner: Finishing task: attempt_local829939055_0001_r_000000_0\n",
      "2018-09-02 20:18:42,731 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2018-09-02 20:18:43,160 INFO mapreduce.Job: Job job_local829939055_0001 running in uber mode : false\n",
      "2018-09-02 20:18:43,161 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2018-09-02 20:18:43,162 INFO mapreduce.Job: Job job_local829939055_0001 completed successfully\n",
      "2018-09-02 20:18:43,172 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=21277\n",
      "\t\tFILE: Number of bytes written=2008296\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6387\n",
      "\t\tHDFS: Number of bytes written=1649\n",
      "\t\tHDFS: Number of read operations=35\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24\n",
      "\t\tMap output records=252\n",
      "\t\tMap output bytes=2873\n",
      "\t\tMap output materialized bytes=2621\n",
      "\t\tInput split bytes=348\n",
      "\t\tCombine input records=252\n",
      "\t\tCombine output records=187\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=2621\n",
      "\t\tReduce input records=187\n",
      "\t\tReduce output records=159\n",
      "\t\tSpilled Records=374\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1801977856\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1882\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1649\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hadoop jar wc.jar WordCount input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 8\n",
    "\n",
    "Se visualiza la salida del programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:18:49,421 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 jdvelasq supergroup          0 2018-09-02 20:18 output/_SUCCESS\n",
      "-rw-r--r--   1 jdvelasq supergroup       1649 2018-09-02 20:18 output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "## Contenido del directorio output\n",
    "!$HADOOP_HOME/bin/hadoop fs -ls output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:18:56,382 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Specifically,\t1\n"
     ]
    }
   ],
   "source": [
    "## imprime el resultado en pantalla\n",
    "!$HADOOP_HOME/bin/hadoop fs -cat output/part-r-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Ejercicio.--** Copie el resultado de la corrida al directorio actual.\n",
    "\n",
    "\n",
    "**Ejercicio.--** Cómo podría mejorar el código anterior? realice una implementación.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 20:19:22,130 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted input/text0.txt\n",
      "Deleted input/text1.txt\n",
      "Deleted input/text2.txt\n",
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-r-00000\n",
      "2018-09-02 20:19:23,851 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## se limpia el directoroio de trabajo\n",
    "!rm WordCount*.* *.jar\n",
    "!rm -rf input output\n",
    "!$HADOOP_HOME/bin/hadoop fs -rm input/* output/*\n",
    "!$HADOOP_HOME/bin/hadoop fs -rmdir input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop/MapReduce -- WordCount en Java (Modo Pseudo)\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/apache-hadoop-course/tree/master/) para acceder al repositorio online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/apache-hadoop-course/tree/master/) para explorar el repositorio usando `nbviewer`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
