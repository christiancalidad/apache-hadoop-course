{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop/MapReduce -- WordCount en Python (Hadoop Streaming)\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/big-data-analytics/tree/master/) para acceder al repositorio online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/big-data-analytics/tree/master/) para explorar el repositorio usando `nbviewer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del problema\n",
    "\n",
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos. Debido a los requerimientos de diseño (gran volúmen de datos y tiempos rápidos de respuesta) se desea implementar una arquitectura Big Data. Se desea implementar la solución en **Python** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se crea el directorio de entrada\n",
    "!rm -rf input\n",
    "!rm -rf output\n",
    "!mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de la implementación\n",
    "\n",
    "Para realizar la implementación solicitada se usará Hadoop Streaming, el cual permite la implementación de aplicaciones MapReduce en cualquier lenguaje que soporte la creación de scripts. Para este caso se ejemplificará la implementación en Python. Hadoop streaming debe su nombre a que usa los streamings de Unix para la entrada y salida en texto (stdin, stdout y stderr respectivamente). El sistema lee y escribe líneas de texto una a una a los respectivos flujos de entrada y salida, tal como ocurre de forma estándar en las herramientas de la línea de comandos de Linux. \n",
    "\n",
    "La implementación requiere un programa para realizar el mapeo y otro para la reducción que se ejecutan independientemente; Hadoop se encarga de la coordinación entre ellos. El intercambio de información se hace a traves de texto, que es el lenguaje universal para intercambio de información. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#! /usr/bin/env python\n",
    "\n",
    "##\n",
    "## Esta es la función que mapea la entrada a parejas (clave, valor)\n",
    "##\n",
    "import sys\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    ##\n",
    "    ## itera sobre cada linea de código recibida\n",
    "    ## a través del flujo de entrada\n",
    "    ##\n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        ##\n",
    "        ## genera las tuplas palabra \\tabulador 1\n",
    "        ## ya que es un conteo de palabras\n",
    "        ##\n",
    "        for word in line.split(): \n",
    "                   \n",
    "            ##\n",
    "            ## escribe al flujo estándar de salida\n",
    "            ##\n",
    "            sys.stdout.write(\"{}\\t1\\n\".format(word))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## El programa anterior se hace ejecutable\n",
    "!chmod +x mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics\t1\r\n",
      "is\t1\r\n",
      "the\t1\r\n",
      "discovery,\t1\r\n",
      "interpretation,\t1\r\n",
      "and\t1\r\n",
      "communication\t1\r\n",
      "of\t1\r\n",
      "meaningful\t1\r\n",
      "patterns\t1\r\n"
     ]
    }
   ],
   "source": [
    "## la salida de la función anterior es:\n",
    "!cat ./input/text*.txt | ./mapper.py | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "##\n",
    "## Esta función reduce los elementos que \n",
    "## tienen la misma clave\n",
    "##\n",
    "\n",
    "if __name__ == '__main__': \n",
    "  \n",
    "    curkey = None\n",
    "    total = 0\n",
    "    \n",
    "    ##\n",
    "    ## cada linea de texto recibida es una \n",
    "    ## entrada clave \\tabulador valor\n",
    "    ##\n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        key, val = line.split(\"\\t\") \n",
    "        val = int(val)\n",
    "        \n",
    "        if key == curkey: \n",
    "            ##\n",
    "            ## No se ha cambiado de clave. Aca se \n",
    "            ## acumulan los valores para la misma clave.\n",
    "            ##\n",
    "            total += val  \n",
    "        else:\n",
    "            ##\n",
    "            ## Se cambio de clave. Se reinicia el\n",
    "            ## acumulador.\n",
    "            ##\n",
    "            if curkey is not None:\n",
    "                ##\n",
    "                ## una vez se han reducido todos los elementos\n",
    "                ## con la misma clave se imprime el resultado en\n",
    "                ## el flujo de salida\n",
    "                ##\n",
    "                sys.stdout.write(\"{}\\t{}\\n\".format(curkey, total)) \n",
    "            \n",
    "            curkey = key\n",
    "            total = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DA)\t1\r\n",
      "(see\t1\r\n",
      "Analytics\t2\r\n",
      "Analytics,\t1\r\n",
      "Big\t1\r\n",
      "Data\t2\r\n",
      "Especially\t1\r\n",
      "Organizations\t1\r\n",
      "Since\t1\r\n",
      "Specifically,\t1\r\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## La función sort hace que todos los elementos con \n",
    "## la misma clave queden en lineas consecutivas.\n",
    "## Hace el papel del módulo Shuffle & Sort\n",
    "##\n",
    "!cat ./input/text*.txt | ./mapper.py | sort | ./reducer.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución en Hadoop\n",
    "\n",
    "Una vez se tienen las versiones anteriores funcionando, se puede proceder a ejecutar la tarea directamente en hadoop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primero se limpian los directorios.\n",
    "!rm -rf output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-27 20:34:34,881 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-08-27 20:34:35,283 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "2018-08-27 20:34:35,341 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2018-08-27 20:34:35,342 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2018-08-27 20:34:35,358 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-08-27 20:34:35,639 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2018-08-27 20:34:35,689 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2018-08-27 20:34:35,838 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local301458228_0001\n",
      "2018-08-27 20:34:35,839 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2018-08-27 20:34:35,939 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2018-08-27 20:34:35,940 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2018-08-27 20:34:35,940 INFO mapreduce.Job: Running job: job_local301458228_0001\n",
      "2018-08-27 20:34:35,941 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2018-08-27 20:34:35,944 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-08-27 20:34:35,944 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-08-27 20:34:35,980 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2018-08-27 20:34:35,983 INFO mapred.LocalJobRunner: Starting task: attempt_local301458228_0001_m_000000_0\n",
      "2018-08-27 20:34:36,000 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-08-27 20:34:36,000 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-08-27 20:34:36,006 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-08-27 20:34:36,006 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-08-27 20:34:36,012 INFO mapred.MapTask: Processing split: file:/Volumes/JetDrive/GitHub/big-data-analytics/input/text0.txt:0+1092\n",
      "2018-08-27 20:34:36,018 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-08-27 20:34:36,080 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-08-27 20:34:36,080 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-08-27 20:34:36,080 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-08-27 20:34:36,080 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-08-27 20:34:36,080 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-08-27 20:34:36,082 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-08-27 20:34:36,089 INFO streaming.PipeMapRed: PipeMapRed exec [/Volumes/JetDrive/GitHub/big-data-analytics/./mapper.py]\n",
      "2018-08-27 20:34:36,092 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2018-08-27 20:34:36,093 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2018-08-27 20:34:36,093 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2018-08-27 20:34:36,093 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2018-08-27 20:34:36,094 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2018-08-27 20:34:36,094 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2018-08-27 20:34:36,094 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2018-08-27 20:34:36,094 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-08-27 20:34:36,094 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2018-08-27 20:34:36,095 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2018-08-27 20:34:36,095 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2018-08-27 20:34:36,095 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2018-08-27 20:34:36,108 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-08-27 20:34:36,108 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-08-27 20:34:36,142 INFO streaming.PipeMapRed: Records R/W=14/1\n",
      "2018-08-27 20:34:36,144 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-08-27 20:34:36,144 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-08-27 20:34:36,146 INFO mapred.LocalJobRunner: \n",
      "2018-08-27 20:34:36,146 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-08-27 20:34:36,146 INFO mapred.MapTask: Spilling map output\n",
      "2018-08-27 20:34:36,146 INFO mapred.MapTask: bufstart = 0; bufend = 1347; bufvoid = 104857600\n",
      "2018-08-27 20:34:36,146 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213868(104855472); length = 529/6553600\n",
      "2018-08-27 20:34:36,168 INFO mapred.MapTask: Finished spill 0\n",
      "2018-08-27 20:34:36,180 INFO mapred.Task: Task:attempt_local301458228_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-08-27 20:34:36,182 INFO mapred.LocalJobRunner: Records R/W=14/1\n",
      "2018-08-27 20:34:36,182 INFO mapred.Task: Task 'attempt_local301458228_0001_m_000000_0' done.\n",
      "2018-08-27 20:34:36,189 INFO mapred.Task: Final Counters for attempt_local301458228_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=177838\n",
      "\t\tFILE: Number of bytes written=680249\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=14\n",
      "\t\tMap output records=133\n",
      "\t\tMap output bytes=1347\n",
      "\t\tMap output materialized bytes=1619\n",
      "\t\tInput split bytes=116\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=133\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=197132288\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1092\n",
      "2018-08-27 20:34:36,189 INFO mapred.LocalJobRunner: Finishing task: attempt_local301458228_0001_m_000000_0\n",
      "2018-08-27 20:34:36,190 INFO mapred.LocalJobRunner: Starting task: attempt_local301458228_0001_m_000001_0\n",
      "2018-08-27 20:34:36,217 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-08-27 20:34:36,217 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-08-27 20:34:36,217 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-08-27 20:34:36,217 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-08-27 20:34:36,218 INFO mapred.MapTask: Processing split: file:/Volumes/JetDrive/GitHub/big-data-analytics/input/text2.txt:0+439\n",
      "2018-08-27 20:34:36,219 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-08-27 20:34:36,230 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-08-27 20:34:36,230 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-08-27 20:34:36,230 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-08-27 20:34:36,230 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-08-27 20:34:36,230 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-08-27 20:34:36,230 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-08-27 20:34:36,235 INFO streaming.PipeMapRed: PipeMapRed exec [/Volumes/JetDrive/GitHub/big-data-analytics/./mapper.py]\n",
      "2018-08-27 20:34:36,244 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-08-27 20:34:36,284 INFO streaming.PipeMapRed: Records R/W=6/1\n",
      "2018-08-27 20:34:36,288 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-08-27 20:34:36,288 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-08-27 20:34:36,288 INFO mapred.LocalJobRunner: \n",
      "2018-08-27 20:34:36,288 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-08-27 20:34:36,288 INFO mapred.MapTask: Spilling map output\n",
      "2018-08-27 20:34:36,288 INFO mapred.MapTask: bufstart = 0; bufend = 559; bufvoid = 104857600\n",
      "2018-08-27 20:34:36,288 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600\n",
      "2018-08-27 20:34:36,300 INFO mapred.MapTask: Finished spill 0\n",
      "2018-08-27 20:34:36,306 INFO mapred.Task: Task:attempt_local301458228_0001_m_000001_0 is done. And is in the process of committing\n",
      "2018-08-27 20:34:36,307 INFO mapred.LocalJobRunner: Records R/W=6/1\n",
      "2018-08-27 20:34:36,307 INFO mapred.Task: Task 'attempt_local301458228_0001_m_000001_0' done.\n",
      "2018-08-27 20:34:36,307 INFO mapred.Task: Final Counters for attempt_local301458228_0001_m_000001_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=178644\n",
      "\t\tFILE: Number of bytes written=680970\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=62\n",
      "\t\tMap output bytes=559\n",
      "\t\tMap output materialized bytes=689\n",
      "\t\tInput split bytes=116\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=62\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=26\n",
      "\t\tTotal committed heap usage (bytes)=296222720\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=439\n",
      "2018-08-27 20:34:36,307 INFO mapred.LocalJobRunner: Finishing task: attempt_local301458228_0001_m_000001_0\n",
      "2018-08-27 20:34:36,308 INFO mapred.LocalJobRunner: Starting task: attempt_local301458228_0001_m_000002_0\n",
      "2018-08-27 20:34:36,308 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-08-27 20:34:36,308 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-08-27 20:34:36,309 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-08-27 20:34:36,309 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-08-27 20:34:36,309 INFO mapred.MapTask: Processing split: file:/Volumes/JetDrive/GitHub/big-data-analytics/input/text1.txt:0+351\n",
      "2018-08-27 20:34:36,310 INFO mapred.MapTask: numReduceTasks: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-27 20:34:36,368 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-08-27 20:34:36,368 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-08-27 20:34:36,368 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-08-27 20:34:36,368 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-08-27 20:34:36,368 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-08-27 20:34:36,369 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-08-27 20:34:36,374 INFO streaming.PipeMapRed: PipeMapRed exec [/Volumes/JetDrive/GitHub/big-data-analytics/./mapper.py]\n",
      "2018-08-27 20:34:36,381 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-08-27 20:34:36,421 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "2018-08-27 20:34:36,424 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-08-27 20:34:36,425 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-08-27 20:34:36,425 INFO mapred.LocalJobRunner: \n",
      "2018-08-27 20:34:36,425 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-08-27 20:34:36,425 INFO mapred.MapTask: Spilling map output\n",
      "2018-08-27 20:34:36,425 INFO mapred.MapTask: bufstart = 0; bufend = 463; bufvoid = 104857600\n",
      "2018-08-27 20:34:36,425 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214172(104856688); length = 225/6553600\n",
      "2018-08-27 20:34:36,436 INFO mapred.MapTask: Finished spill 0\n",
      "2018-08-27 20:34:36,443 INFO mapred.Task: Task:attempt_local301458228_0001_m_000002_0 is done. And is in the process of committing\n",
      "2018-08-27 20:34:36,444 INFO mapred.LocalJobRunner: Records R/W=4/1\n",
      "2018-08-27 20:34:36,444 INFO mapred.Task: Task 'attempt_local301458228_0001_m_000002_0' done.\n",
      "2018-08-27 20:34:36,444 INFO mapred.Task: Final Counters for attempt_local301458228_0001_m_000002_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=179362\n",
      "\t\tFILE: Number of bytes written=681585\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=57\n",
      "\t\tMap output bytes=463\n",
      "\t\tMap output materialized bytes=583\n",
      "\t\tInput split bytes=116\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=57\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=401604608\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=351\n",
      "2018-08-27 20:34:36,444 INFO mapred.LocalJobRunner: Finishing task: attempt_local301458228_0001_m_000002_0\n",
      "2018-08-27 20:34:36,444 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2018-08-27 20:34:36,446 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2018-08-27 20:34:36,447 INFO mapred.LocalJobRunner: Starting task: attempt_local301458228_0001_r_000000_0\n",
      "2018-08-27 20:34:36,452 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-08-27 20:34:36,452 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-08-27 20:34:36,452 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-08-27 20:34:36,452 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-08-27 20:34:36,454 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@563303c9\n",
      "2018-08-27 20:34:36,455 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-08-27 20:34:36,466 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-08-27 20:34:36,468 INFO reduce.EventFetcher: attempt_local301458228_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-08-27 20:34:36,485 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local301458228_0001_m_000000_0 decomp: 1615 len: 1619 to MEMORY\n",
      "2018-08-27 20:34:36,491 INFO reduce.InMemoryMapOutput: Read 1615 bytes from map-output for attempt_local301458228_0001_m_000000_0\n",
      "2018-08-27 20:34:36,492 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1615, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1615\n",
      "2018-08-27 20:34:36,494 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local301458228_0001_m_000001_0 decomp: 685 len: 689 to MEMORY\n",
      "2018-08-27 20:34:36,495 INFO reduce.InMemoryMapOutput: Read 685 bytes from map-output for attempt_local301458228_0001_m_000001_0\n",
      "2018-08-27 20:34:36,495 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 685, inMemoryMapOutputs.size() -> 2, commitMemory -> 1615, usedMemory ->2300\n",
      "2018-08-27 20:34:36,496 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local301458228_0001_m_000002_0 decomp: 579 len: 583 to MEMORY\n",
      "2018-08-27 20:34:36,496 INFO reduce.InMemoryMapOutput: Read 579 bytes from map-output for attempt_local301458228_0001_m_000002_0\n",
      "2018-08-27 20:34:36,496 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 579, inMemoryMapOutputs.size() -> 3, commitMemory -> 2300, usedMemory ->2879\n",
      "2018-08-27 20:34:36,496 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2018-08-27 20:34:36,497 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-08-27 20:34:36,497 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-08-27 20:34:36,506 INFO mapred.Merger: Merging 3 sorted segments\n",
      "2018-08-27 20:34:36,507 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 2853 bytes\n",
      "2018-08-27 20:34:36,513 INFO reduce.MergeManagerImpl: Merged 3 segments, 2879 bytes to disk to satisfy reduce memory limit\n",
      "2018-08-27 20:34:36,513 INFO reduce.MergeManagerImpl: Merging 1 files, 2879 bytes from disk\n",
      "2018-08-27 20:34:36,514 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-08-27 20:34:36,514 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-08-27 20:34:36,514 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2868 bytes\n",
      "2018-08-27 20:34:36,515 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-08-27 20:34:36,520 INFO streaming.PipeMapRed: PipeMapRed exec [/Volumes/JetDrive/GitHub/big-data-analytics/./reducer.py]\n",
      "2018-08-27 20:34:36,521 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-08-27 20:34:36,522 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2018-08-27 20:34:36,554 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-08-27 20:34:36,555 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-08-27 20:34:36,556 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-08-27 20:34:36,569 INFO streaming.PipeMapRed: Records R/W=252/1\n",
      "2018-08-27 20:34:36,572 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-08-27 20:34:36,572 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-08-27 20:34:36,573 INFO mapred.Task: Task:attempt_local301458228_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-08-27 20:34:36,573 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "2018-08-27 20:34:36,573 INFO mapred.Task: Task attempt_local301458228_0001_r_000000_0 is allowed to commit now\n",
      "2018-08-27 20:34:36,575 INFO output.FileOutputCommitter: Saved output of task 'attempt_local301458228_0001_r_000000_0' to file:/Volumes/JetDrive/GitHub/big-data-analytics/output\n",
      "2018-08-27 20:34:36,575 INFO mapred.LocalJobRunner: Records R/W=252/1 > reduce\n",
      "2018-08-27 20:34:36,575 INFO mapred.Task: Task 'attempt_local301458228_0001_r_000000_0' done.\n",
      "2018-08-27 20:34:36,576 INFO mapred.Task: Final Counters for attempt_local301458228_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=185228\n",
      "\t\tFILE: Number of bytes written=686128\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=2891\n",
      "\t\tReduce input records=252\n",
      "\t\tReduce output records=158\n",
      "\t\tSpilled Records=252\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=401604608\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1664\n",
      "2018-08-27 20:34:36,576 INFO mapred.LocalJobRunner: Finishing task: attempt_local301458228_0001_r_000000_0\n",
      "2018-08-27 20:34:36,576 INFO mapred.LocalJobRunner: reduce task executor complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-27 20:34:36,949 INFO mapreduce.Job: Job job_local301458228_0001 running in uber mode : false\r\n",
      "2018-08-27 20:34:36,950 INFO mapreduce.Job:  map 100% reduce 100%\r\n",
      "2018-08-27 20:34:36,951 INFO mapreduce.Job: Job job_local301458228_0001 completed successfully\r\n",
      "2018-08-27 20:34:36,959 INFO mapreduce.Job: Counters: 30\r\n",
      "\tFile System Counters\r\n",
      "\t\tFILE: Number of bytes read=721072\r\n",
      "\t\tFILE: Number of bytes written=2728932\r\n",
      "\t\tFILE: Number of read operations=0\r\n",
      "\t\tFILE: Number of large read operations=0\r\n",
      "\t\tFILE: Number of write operations=0\r\n",
      "\tMap-Reduce Framework\r\n",
      "\t\tMap input records=24\r\n",
      "\t\tMap output records=252\r\n",
      "\t\tMap output bytes=2369\r\n",
      "\t\tMap output materialized bytes=2891\r\n",
      "\t\tInput split bytes=348\r\n",
      "\t\tCombine input records=0\r\n",
      "\t\tCombine output records=0\r\n",
      "\t\tReduce input groups=159\r\n",
      "\t\tReduce shuffle bytes=2891\r\n",
      "\t\tReduce input records=252\r\n",
      "\t\tReduce output records=158\r\n",
      "\t\tSpilled Records=504\r\n",
      "\t\tShuffled Maps =3\r\n",
      "\t\tFailed Shuffles=0\r\n",
      "\t\tMerged Map outputs=3\r\n",
      "\t\tGC time elapsed (ms)=26\r\n",
      "\t\tTotal committed heap usage (bytes)=1296564224\r\n",
      "\tShuffle Errors\r\n",
      "\t\tBAD_ID=0\r\n",
      "\t\tCONNECTION=0\r\n",
      "\t\tIO_ERROR=0\r\n",
      "\t\tWRONG_LENGTH=0\r\n",
      "\t\tWRONG_MAP=0\r\n",
      "\t\tWRONG_REDUCE=0\r\n",
      "\tFile Input Format Counters \r\n",
      "\t\tBytes Read=1882\r\n",
      "\tFile Output Format Counters \r\n",
      "\t\tBytes Written=1664\r\n",
      "2018-08-27 20:34:36,959 INFO streaming.StreamJob: Output directory: output\r\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se ejecuta en Hadoop.\n",
    "##   -input: archivo de entrada\n",
    "##   -output: directorio de salida\n",
    "##   -maper: programa que ejecuta el map\n",
    "##   -reducer: programa que ejecuta la reducción\n",
    "##\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar -input input -output output  -mapper mapper.py -reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS   part-00000\r\n"
     ]
    }
   ],
   "source": [
    "## contenido del directorio con los \n",
    "## resultados de la corrida\n",
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DA)\t1\r\n",
      "(see\t1\r\n",
      "Analytics\t2\r\n",
      "Analytics,\t1\r\n",
      "Big\t1\r\n",
      "Data\t3\r\n",
      "Especially\t1\r\n",
      "Organizations\t1\r\n",
      "Since\t1\r\n",
      "Specifically,\t1\r\n"
     ]
    }
   ],
   "source": [
    "## se visualiza el archivo con los\n",
    "## resultados de la corrida\n",
    "!cat output/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm reducer.py mapper.py\n",
    "!rm -rf input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas.\n",
    "\n",
    "**Combiners.--** Los combiners son *reducers* que se ejecutan sobre los resultdos que produce cada mapper antes de pasar al modulo de suffle-&-sort, con el fin de reducir la carga computacional. Suelen ser identicos a los *reducers*.\n",
    "\n",
    "**Partitioners.--** Son rutinas que controlan como se enviar las parejas (clave, valor) a cada reducers, tal que elementos con la misma clave son enviados al mismo reducer. \n",
    "\n",
    "**Job Chain.--** Se refiere al encadenamiento de varias tareas cuando el cómputo que se desea realizar es muy complejo para que pueda realizarse en un MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop/MapReduce -- WordCount en Python (Hadoop Streaming)\n",
    "===\n",
    "\n",
    "**Juan David Velásquez Henao**  \n",
    "jdvelasq@unal.edu.co   \n",
    "Universidad Nacional de Colombia, Sede Medellín  \n",
    "Facultad de Minas  \n",
    "Medellín, Colombia\n",
    "\n",
    "---\n",
    "\n",
    "Haga click [aquí](https://github.com/jdvelasq/big-data-analytics/tree/master/) para acceder al repositorio online.\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/github/jdvelasq/big-data-analytics/tree/master/) para explorar el repositorio usando `nbviewer`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
